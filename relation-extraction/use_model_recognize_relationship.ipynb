{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/data/t200404/software/anaconda3/envs/python3_11_gpu/lib/python3.11/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/data/t200404/software/anaconda3/envs/python3_11_gpu/lib/python3.11/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import sys\n",
    "\n",
    "def load_model_and_tokenizer(model_dir, device):\n",
    "    # 加载配置文件、分词器和模型\n",
    "    config = AutoConfig.from_pretrained(model_dir)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    return config, tokenizer, model\n",
    "\n",
    "def predict(texts, tokenizer, model, label_map, device):\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "    \n",
    "    predictions = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "    return [label_map[pred] for pred in predictions]\n",
    "\n",
    "model_dir = './output/GAD-1'  # 从命令行参数获取模型目录\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "config, tokenizer, model = load_model_and_tokenizer(model_dir, device)\n",
    "label_map = config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示例输入文本\n",
    "texts = [\n",
    "    \"The @GENE$ gene is less likely to play a substantial role in the development of atopy and @DISEASE$ in the Japanese population.\",\n",
    "    \"The PC36:0  is less likely to play a substantial role in the development of atopy and @DISEASE$ in the Japanese population.\"\n",
    "]\n",
    "\n",
    "# 进行预测\n",
    "predictions = predict(texts, tokenizer, model, label_map, device)\n",
    "\n",
    "# 打印预测结果\n",
    "for text, prediction in zip(texts, predictions):\n",
    "    print(f\"Text: {text}\\nPrediction: {prediction}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "ner_extract_dir = '/home/data/t200404/bioinfo/P_subject/NLP/biobert/datasets/for_recognize/download_paper_and_use_Auto-CORPus_deal_paper/deal/extract_result/'\n",
    "ner_extract_file = ner_extract_dir +'df_dict_lipid_and_disease_4_combine_lipid_disease_ture_combine_2.pkl'\n",
    "with open(ner_extract_file,'rb') as f:\n",
    "    df_dict = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iao_name_1</th>\n",
       "      <th>iao_id_1</th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>split_sentence</th>\n",
       "      <th>predictions</th>\n",
       "      <th>predictions_lipid</th>\n",
       "      <th>predictions_disease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>document title</td>\n",
       "      <td>IAO:0000305</td>\n",
       "      <td>Cationic amphiphilic drugs induce accumulation...</td>\n",
       "      <td>1</td>\n",
       "      <td>Cationic\\namphiphilic\\ndrugs\\ninduce\\naccumula...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>textual abstract section</td>\n",
       "      <td>IAO:0000315</td>\n",
       "      <td>Lysosomes are acidic organelles responsible fo...</td>\n",
       "      <td>1</td>\n",
       "      <td>Lysosomes\\nare\\nacidic\\norganelles\\nresponsibl...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>textual abstract section</td>\n",
       "      <td>IAO:0000315</td>\n",
       "      <td>These drugs can also induce lysosomal membrane...</td>\n",
       "      <td>2</td>\n",
       "      <td>These\\ndrugs\\ncan\\nalso\\ninduce\\nlysosomal\\nme...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{'tokens': ['[CLS]', 'these', 'drugs', 'can', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>textual abstract section</td>\n",
       "      <td>IAO:0000315</td>\n",
       "      <td>Here, we uncover that the cationic amphiphilic...</td>\n",
       "      <td>3</td>\n",
       "      <td>Here,\\nwe\\nuncover\\nthat\\nthe\\ncationic\\namphi...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{'tokens': ['[CLS]', 'here', ',', 'we', 'un', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>textual abstract section</td>\n",
       "      <td>IAO:0000315</td>\n",
       "      <td>Using quantitative mass spectrometry-based sho...</td>\n",
       "      <td>4</td>\n",
       "      <td>Using\\nquantitative\\nmass\\nspectrometry-based\\...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{'tokens': ['[CLS]', 'using', 'quantitative', ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 iao_name_1     iao_id_1  \\\n",
       "0            document title  IAO:0000305   \n",
       "1  textual abstract section  IAO:0000315   \n",
       "2  textual abstract section  IAO:0000315   \n",
       "3  textual abstract section  IAO:0000315   \n",
       "4  textual abstract section  IAO:0000315   \n",
       "\n",
       "                                            sentence  sentence_number  \\\n",
       "0  Cationic amphiphilic drugs induce accumulation...                1   \n",
       "1  Lysosomes are acidic organelles responsible fo...                1   \n",
       "2  These drugs can also induce lysosomal membrane...                2   \n",
       "3  Here, we uncover that the cationic amphiphilic...                3   \n",
       "4  Using quantitative mass spectrometry-based sho...                4   \n",
       "\n",
       "                                      split_sentence predictions  \\\n",
       "0  Cationic\\namphiphilic\\ndrugs\\ninduce\\naccumula...        None   \n",
       "1  Lysosomes\\nare\\nacidic\\norganelles\\nresponsibl...        None   \n",
       "2  These\\ndrugs\\ncan\\nalso\\ninduce\\nlysosomal\\nme...        None   \n",
       "3  Here,\\nwe\\nuncover\\nthat\\nthe\\ncationic\\namphi...        None   \n",
       "4  Using\\nquantitative\\nmass\\nspectrometry-based\\...        None   \n",
       "\n",
       "  predictions_lipid                                predictions_disease  \n",
       "0              None                                               None  \n",
       "1              None                                               None  \n",
       "2              None  {'tokens': ['[CLS]', 'these', 'drugs', 'can', ...  \n",
       "3              None  {'tokens': ['[CLS]', 'here', ',', 'we', 'un', ...  \n",
       "4              None  {'tokens': ['[CLS]', 'using', 'quantitative', ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ = df_dict['PMC10916870_bioc.json']\n",
    "df_.head()\n",
    "# df_['have_lipid_and_disease'] = ((df_['predictions_lipid'].notna()) & (df_['predictions_disease'].notna())).astype(int)\n",
    "# filtered_df = df_[df_['have_lipid_and_disease'] == 1]\n",
    "# filtered_df.head()\n",
    "df_[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These drugs can also induce lysosomal membrane permeabilization and cancer cell death, but the underlying mechanism remains elusive.\n",
      "{'tokens': ['[CLS]', 'these', 'drugs', 'can', 'also', 'induce', 'l', '##ys', '##oso', '##mal', 'membrane', 'per', '##me', '##abi', '##li', '##zation', 'and', 'cancer', 'cell', 'death', ',', 'but', 'the', 'underlying', 'mechanism', 'remains', 'el', '##usive', '.', '[SEP]'], 'labels': ['B-disease']}\n"
     ]
    }
   ],
   "source": [
    "sentence= df_.iloc[2]['sentence']\n",
    "print(sentence)\n",
    "\n",
    "tokens2label = df_.iloc[2]['predictions_disease']\n",
    "print(tokens2label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokens2label['tokens'])\n",
    "tokens2label['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.encode(tokens2label['tokens'])\n",
    "# tokenizer.convert_ids_to_tokens(tokens2label['tokens'])\n",
    "\n",
    "aa = tokenizer.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = config.id2label\n",
    "for PMC_bioc_name in df_dict.keys():    \n",
    "    if 'predictions_lipid' in df_dict[PMC_bioc_name].columns:\n",
    "        continue\n",
    "    else:\n",
    "        print(PMC_bioc_name)\n",
    "        results = df_dict[PMC_bioc_name]['sentence'].apply(\n",
    "            lambda x: predict(x, tokenizer, model, label_map, device='cuda')\n",
    "        )\n",
    "        \n",
    "        df_dict[PMC_bioc_name]['predictions_lipid'], df_dict[PMC_bioc_name]['predictions_disease'] = zip(*results)\n",
    "        with open(f'df_dict_lipid_and_disease_{model_name}.pkl','wb') as f:\n",
    "            pickle.dump(df_dict, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]: O\n",
      "our: O\n",
      "previous: O\n",
      "studies: O\n",
      "have: O\n",
      "shown: O\n",
      "that: O\n",
      "cad: O\n",
      "-: O\n",
      "induced: O\n",
      "lmp: O\n",
      "and: O\n",
      "cell: O\n",
      "death: O\n",
      "rely: O\n",
      "on: O\n",
      "the: O\n",
      "inhibition: O\n",
      "of: O\n",
      "lysosomal: O\n",
      "acid: O\n",
      "sphingomyelinase: O\n",
      "(: O\n",
      "asm: O\n",
      "/: O\n",
      "smpd1: O\n",
      ";: O\n",
      "petersen: O\n",
      "et: O\n",
      "al: O\n",
      ".: O\n",
      ",: O\n",
      "2013: O\n",
      "): O\n",
      ",: O\n",
      "while: O\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# 示例数据\n",
    "tokens = ['[CLS]', 'our', 'previous', 'studies', 'have', 'shown', 'that', 'ca', '##d', '-', 'induced', 'l', '##mp', 'and', 'cell', 'death', 'rely', 'on', 'the', 'in', '##hibition', 'of', 'l', '##ys', '##oso', '##mal', 'acid', 's', '##phi', '##ngo', '##my', '##elin', '##ase', '(', 'as', '##m', '/', 's', '##mp', '##d', '##1', ';', 'pet', '##erse', '##n', 'et', 'al', '.', ',', '2013', ')', ',', 'while']\n",
    "labels = ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
    "\n",
    "# 将子词组合成完整的单词，并为每个完整的单词分配标签\n",
    "def combine_subwords(tokens, labels):\n",
    "    combined_tokens = []\n",
    "    combined_labels = []\n",
    "    \n",
    "    current_token = \"\"\n",
    "    current_label = labels[0]\n",
    "    \n",
    "    for token, label in zip(tokens, labels):\n",
    "        if token.startswith(\"##\"):\n",
    "            current_token += token[2:]\n",
    "        else:\n",
    "            if current_token:\n",
    "                combined_tokens.append(current_token)\n",
    "                combined_labels.append(current_label)\n",
    "            current_token = token\n",
    "            current_label = label\n",
    "    \n",
    "    # 添加最后一个 token\n",
    "    if current_token:\n",
    "        combined_tokens.append(current_token)\n",
    "        combined_labels.append(current_label)\n",
    "    \n",
    "    return combined_tokens, combined_labels\n",
    "\n",
    "# 执行函数\n",
    "combined_tokens, combined_labels = combine_subwords(tokens, labels)\n",
    "\n",
    "# 输出结果\n",
    "for token, label in zip(combined_tokens, combined_labels):\n",
    "    print(f\"{token}: {label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'our',\n",
       " 'previous',\n",
       " 'studies',\n",
       " 'have',\n",
       " 'shown',\n",
       " 'that',\n",
       " 'cad',\n",
       " '-',\n",
       " 'induced',\n",
       " 'lmp',\n",
       " 'and',\n",
       " 'cell',\n",
       " 'death',\n",
       " 'rely',\n",
       " 'on',\n",
       " 'the',\n",
       " 'inhibition',\n",
       " 'of',\n",
       " 'lysosomal',\n",
       " 'acid',\n",
       " 'sphingomyelinase',\n",
       " '(',\n",
       " 'asm',\n",
       " '/',\n",
       " 'smpd1',\n",
       " ';',\n",
       " 'petersen',\n",
       " 'et',\n",
       " 'al',\n",
       " '.',\n",
       " ',',\n",
       " '2013',\n",
       " ')',\n",
       " ',',\n",
       " 'while']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import RobertaTokenizerFast\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import sys\n",
    "config = AutoConfig.from_pretrained(model_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
    "\n",
    "# tokenizer = RobertaTokenizerFast.from_pretrained('roberta-large')\n",
    "\n",
    "example = \"This is a tokenization example\"\n",
    "\n",
    "enc = tokenizer(example, add_special_tokens=False)\n",
    "\n",
    "desired_output = []\n",
    "\n",
    "#BatchEncoding.word_ids returns a list mapping words to tokens\n",
    "for w_idx in set(enc.word_ids()):\n",
    "    #BatchEncoding.word_to_tokens tells us which and how many tokens are used for the specific word\n",
    "    start, end = enc.word_to_tokens(w_idx)\n",
    "    # we add +1 because you wanted to start with 1 and not with 0\n",
    "    start+=1\n",
    "    end+=1\n",
    "    desired_output.append(list(range(start,end)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_ids': [1142, 1110, 170, 22559, 2734, 1859], 'token_type_ids': [0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1]},\n",
       " [[1], [2], [3], [4, 5], [6]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc , desired_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "PreTrainedTokenizerFast._batch_encode_plus() got an unexpected keyword argument 'add_prefix_space'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m({x : tokenizer\u001b[38;5;241m.\u001b[39mencode(x, add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, add_prefix_space\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m example\u001b[38;5;241m.\u001b[39msplit()})\n",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m({x : tokenizer\u001b[38;5;241m.\u001b[39mencode(x, add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, add_prefix_space\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m example\u001b[38;5;241m.\u001b[39msplit()})\n",
      "File \u001b[0;32m~/software/anaconda3/envs/python3_11_gpu/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2373\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m   2336\u001b[0m \u001b[38;5;129m@add_end_docstrings\u001b[39m(\n\u001b[1;32m   2337\u001b[0m     ENCODE_KWARGS_DOCSTRING,\n\u001b[1;32m   2338\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2356\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2357\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m   2358\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2359\u001b[0m \u001b[38;5;124;03m    Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.\u001b[39;00m\n\u001b[1;32m   2360\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2371\u001b[0m \u001b[38;5;124;03m            method).\u001b[39;00m\n\u001b[1;32m   2372\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2373\u001b[0m     encoded_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[1;32m   2374\u001b[0m         text,\n\u001b[1;32m   2375\u001b[0m         text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[1;32m   2376\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[1;32m   2377\u001b[0m         padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   2378\u001b[0m         truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[1;32m   2379\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[1;32m   2380\u001b[0m         stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[1;32m   2381\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[1;32m   2382\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2383\u001b[0m     )\n\u001b[1;32m   2385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m encoded_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/software/anaconda3/envs/python3_11_gpu/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2781\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2771\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   2772\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   2773\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   2774\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2778\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2779\u001b[0m )\n\u001b[0;32m-> 2781\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encode_plus(\n\u001b[1;32m   2782\u001b[0m     text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[1;32m   2783\u001b[0m     text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[1;32m   2784\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[1;32m   2785\u001b[0m     padding_strategy\u001b[38;5;241m=\u001b[39mpadding_strategy,\n\u001b[1;32m   2786\u001b[0m     truncation_strategy\u001b[38;5;241m=\u001b[39mtruncation_strategy,\n\u001b[1;32m   2787\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[1;32m   2788\u001b[0m     stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[1;32m   2789\u001b[0m     is_split_into_words\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[1;32m   2790\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[1;32m   2791\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[1;32m   2792\u001b[0m     return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[1;32m   2793\u001b[0m     return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[1;32m   2794\u001b[0m     return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[1;32m   2795\u001b[0m     return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[1;32m   2796\u001b[0m     return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[1;32m   2797\u001b[0m     return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[1;32m   2798\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m   2799\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2800\u001b[0m )\n",
      "File \u001b[0;32m~/software/anaconda3/envs/python3_11_gpu/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py:517\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_encode_plus\u001b[39m(\n\u001b[1;32m    496\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    497\u001b[0m     text: Union[TextInput, PreTokenizedInput],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    515\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchEncoding:\n\u001b[1;32m    516\u001b[0m     batched_input \u001b[38;5;241m=\u001b[39m [(text, text_pair)] \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;28;01melse\u001b[39;00m [text]\n\u001b[0;32m--> 517\u001b[0m     batched_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_encode_plus(\n\u001b[1;32m    518\u001b[0m         batched_input,\n\u001b[1;32m    519\u001b[0m         is_split_into_words\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[1;32m    520\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[1;32m    521\u001b[0m         padding_strategy\u001b[38;5;241m=\u001b[39mpadding_strategy,\n\u001b[1;32m    522\u001b[0m         truncation_strategy\u001b[38;5;241m=\u001b[39mtruncation_strategy,\n\u001b[1;32m    523\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[1;32m    524\u001b[0m         stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[1;32m    525\u001b[0m         pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[1;32m    526\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[1;32m    527\u001b[0m         return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[1;32m    528\u001b[0m         return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[1;32m    529\u001b[0m         return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[1;32m    530\u001b[0m         return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[1;32m    531\u001b[0m         return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[1;32m    532\u001b[0m         return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[1;32m    533\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m    534\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    535\u001b[0m     )\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;66;03m# Return tensor is None, then we can remove the leading batch axis\u001b[39;00m\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;66;03m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001b[39;00m\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_tensors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_overflowing_tokens:\n",
      "\u001b[0;31mTypeError\u001b[0m: PreTrainedTokenizerFast._batch_encode_plus() got an unexpected keyword argument 'add_prefix_space'"
     ]
    }
   ],
   "source": [
    "print({x : tokenizer.encode(x, add_special_tokens=False, add_prefix_space=True) for x in example.split()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', 'a', 'tokenization', 'example']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "import sys\n",
    "\n",
    "def load_model_and_tokenizer(model_dir,device = 'cuda'):\n",
    "    # 加载配置文件、分词器和模型\n",
    "    config = AutoConfig.from_pretrained(model_dir)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_dir)\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    return config, tokenizer, model\n",
    "\n",
    "def predict(text, tokenizer, model, label_map, device = 'cuda'):\n",
    "    # 将输入文本分词\n",
    "    text_split = text.split()\n",
    "    inputs = tokenizer(text_split, return_tensors=\"pt\", truncation=True, is_split_into_words=True, max_length=512)\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "    # 使用模型进行预测\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # 兼容不同版本的transformers库\n",
    "        if isinstance(outputs, tuple):\n",
    "            logits = outputs[0]\n",
    "        else:\n",
    "            logits = outputs.logits\n",
    "\n",
    "    # 获取预测结果\n",
    "    predictions = torch.argmax(logits, dim=2)\n",
    "    \n",
    "    predictions = predictions.cpu().detach().numpy()  #gpu中的torch.tensor,需要先把它放进cpu才可以转化\n",
    "    \n",
    "\n",
    "    # 将预测结果转换为标签\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "    predicted_labels = [label_map[pred] for pred in predictions[0]]\n",
    "    prediction ={'tokens':tokens, 'labels':predicted_labels}\n",
    "    # \n",
    "    # 分别提取 lipid 和 disease 标签\n",
    "    if_have_lipid_predictions = list(set(predicted_labels).intersection(['B-lipid','I-lipid']))\n",
    "    if_have_disease_predictions = list(set(predicted_labels).intersection(['B-disease', 'I-disease']))\n",
    "\n",
    "    lipid_result = {'tokens': tokens, 'labels': predicted_labels} if if_have_lipid_predictions else None\n",
    "    disease_result = {'tokens': tokens, 'labels': predicted_labels} if if_have_disease_predictions else None\n",
    "    # print(lipid_result, disease_result)\n",
    "    return lipid_result, disease_result\n",
    "\n",
    "model_dir = '/home/data/t200404/bioinfo/P_subject/NLP/biobert/biobertModelWarehouse/model_from_trained/NER/4_combine_lipid_disease_ture_combine_2'  # 从命令行参数获取模型目录\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "config, tokenizer, model = load_model_and_tokenizer(model_dir, device)\n",
    "label_map = config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Thus, changes in plasma  PC(20:1) levels, plasma S1P d18:1 levels, plasma MonCer d18:1 levels or plasma LacCer d18:1 levels were inferred to be disease-induced changes in Alzheimer's disease or DLB\"\n",
    "# predict(sentence, tokenizer, model, label_map, device='cuda')\n",
    "\n",
    "\n",
    "text_split = text.split()\n",
    "inputs = tokenizer(text_split, return_tensors=\"pt\", truncation=True, is_split_into_words=True, max_length=512)\n",
    "input_ids = inputs[\"input_ids\"].to(device)\n",
    "attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "# 使用模型进行预测\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    # 兼容不同版本的transformers库\n",
    "    if isinstance(outputs, tuple):\n",
    "        logits = outputs[0]\n",
    "    else:\n",
    "        logits = outputs.logits\n",
    "\n",
    "# 获取预测结果\n",
    "predictions = torch.argmax(logits, dim=2)\n",
    "\n",
    "predictions = predictions.cpu().detach().numpy()  #gpu中的torch.tensor,需要先把它放进cpu才可以转化\n",
    "\n",
    "\n",
    "# desired_output = []\n",
    "# for word_id in inputs.word_ids():\n",
    "#     if word_id is not None:\n",
    "#         start, end = inputs.word_to_tokens(word_id)\n",
    "#         if start == end - 1:\n",
    "#             tokens = [start]\n",
    "#         else:\n",
    "#             tokens = [start, end-1]\n",
    "#         if len(desired_output) == 0 or desired_output[-1] != tokens:\n",
    "#             desired_output.append(tokens)\n",
    "\n",
    "# label_list = []\n",
    "# for range_ in  desired_output:\n",
    "#     if len(range_) == 1:\n",
    "#         label_list.append(int(predictions[0][range_]))\n",
    "\n",
    "#     else:\n",
    "#         label_list.append(int(predictions[0][range_[0]:range_[1]+1]))\n",
    "\n",
    "\n",
    "# 将预测结果转换为标签\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "predicted_labels = [label_map[pred] for pred in predictions[0]]\n",
    "prediction ={'tokens':tokens, 'labels':predicted_labels}\n",
    "# \n",
    "# 分别提取 lipid 和 disease 标签\n",
    "if_have_lipid_predictions = list(set(predicted_labels).intersection(['B-lipid','I-lipid']))\n",
    "if_have_disease_predictions = list(set(predicted_labels).intersection(['B-disease', 'I-disease']))\n",
    "\n",
    "lipid_result = {'tokens': tokens, 'labels': predicted_labels} if if_have_lipid_predictions else None\n",
    "disease_result = {'tokens': tokens, 'labels': predicted_labels} if if_have_disease_predictions else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] O\n",
      "thus O\n",
      ", O\n",
      "changes O\n",
      "in O\n",
      "plasma O\n",
      "p B-lipid\n",
      "##c O\n",
      "( I-lipid\n",
      "20 O\n",
      ": O\n",
      "1 O\n",
      ") O\n",
      "levels O\n",
      ", O\n",
      "plasma O\n",
      "s O\n",
      "##1 O\n",
      "##p I-lipid\n",
      "d I-lipid\n",
      "##18 I-lipid\n",
      ": O\n",
      "1 O\n",
      "levels O\n",
      ", O\n",
      "plasma O\n",
      "mon O\n",
      "##cer O\n",
      "d O\n",
      "##18 O\n",
      ": O\n",
      "1 O\n",
      "levels O\n",
      "or O\n",
      "plasma O\n",
      "la B-lipid\n",
      "##cc B-lipid\n",
      "##er O\n",
      "d O\n",
      "##18 O\n",
      ": O\n",
      "1 O\n",
      "levels O\n",
      "were O\n",
      "in O\n",
      "##ferred O\n",
      "to O\n",
      "be O\n",
      "disease O\n",
      "- O\n",
      "induced O\n",
      "changes O\n",
      "in O\n",
      "al B-disease\n",
      "##z I-disease\n",
      "##heimer I-disease\n",
      "' I-disease\n",
      "s I-disease\n",
      "disease I-disease\n",
      "or O\n",
      "d O\n",
      "##l O\n",
      "##b O\n",
      "[SEP] O\n"
     ]
    }
   ],
   "source": [
    "for (token,label) in zip(tokens,predicted_labels):\n",
    "    print(token,label)\n",
    "# tokens, predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['[CLS]',\n",
       "  'thus',\n",
       "  ',',\n",
       "  'changes',\n",
       "  'in',\n",
       "  'plasma',\n",
       "  's',\n",
       "  '##1',\n",
       "  '##p',\n",
       "  'd',\n",
       "  '##16',\n",
       "  ':',\n",
       "  '1',\n",
       "  'levels',\n",
       "  ',',\n",
       "  'plasma',\n",
       "  's',\n",
       "  '##1',\n",
       "  '##p',\n",
       "  'd',\n",
       "  '##18',\n",
       "  ':',\n",
       "  '1',\n",
       "  'levels',\n",
       "  ',',\n",
       "  'plasma',\n",
       "  'mon',\n",
       "  '##cer',\n",
       "  'd',\n",
       "  '##18',\n",
       "  ':',\n",
       "  '1',\n",
       "  'levels',\n",
       "  'or',\n",
       "  'plasma',\n",
       "  'la',\n",
       "  '##cc',\n",
       "  '##er',\n",
       "  'd',\n",
       "  '##18',\n",
       "  ':',\n",
       "  '1',\n",
       "  'levels',\n",
       "  'were',\n",
       "  'in',\n",
       "  '##ferred',\n",
       "  'to',\n",
       "  'be',\n",
       "  'disease',\n",
       "  '-',\n",
       "  'induced',\n",
       "  'changes',\n",
       "  'in',\n",
       "  'al',\n",
       "  '##z',\n",
       "  '##heimer',\n",
       "  \"'\",\n",
       "  's',\n",
       "  'disease',\n",
       "  'or',\n",
       "  'd',\n",
       "  '##l',\n",
       "  '##b',\n",
       "  '[SEP]'],\n",
       " 'labels': ['O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-lipid',\n",
       "  'B-lipid',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-disease',\n",
       "  'I-disease',\n",
       "  'I-disease',\n",
       "  'I-disease',\n",
       "  'I-disease',\n",
       "  'I-disease',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O']}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lipid_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokens2label['tokens'])\n",
    "tokens2label['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_ = predict(sentence, tokenizer, model, label_map, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 18,\n",
       " 19,\n",
       " None]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded.word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-disease',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1292,  5557,  1169,  1145, 21497,   181,  6834, 22354,  7435,\n",
       "         10936,  1679,  3263, 23156,  2646,  8569,  1105,  4182,  2765,  1473,\n",
       "           117,  1133,  1103, 10311,  6978,  2606,  8468, 17849,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs #, input_ids, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokens2label['tokens'])\n",
    "tokens2label['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1],\n",
       " [2],\n",
       " [3],\n",
       " [4],\n",
       " [5],\n",
       " [6, 9],\n",
       " [10],\n",
       " [11, 15],\n",
       " [16],\n",
       " [17],\n",
       " [18],\n",
       " [19],\n",
       " [20],\n",
       " [21],\n",
       " [22],\n",
       " [23],\n",
       " [24],\n",
       " [25],\n",
       " [26, 27],\n",
       " [28]]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = \"These drugs can also induce lysosomal membrane permeabilization and cancer cell death, but the underlying mechanism remains elusive.\"\n",
    "\n",
    "encoded = tokenizer(example)\n",
    "\n",
    "desired_output = []\n",
    "for word_id in encoded.word_ids():\n",
    "    if word_id is not None:\n",
    "        start, end = encoded.word_to_tokens(word_id)\n",
    "        if start == end - 1:\n",
    "            tokens = [start]\n",
    "        else:\n",
    "            tokens = [start, end-1]\n",
    "        if len(desired_output) == 0 or desired_output[-1] != tokens:\n",
    "            desired_output.append(tokens)\n",
    "desired_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = tokenizer(example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1292, 5557, 1169, 1145, 21497, 181, 6834, 22354, 7435, 10936, 1679, 3263, 23156, 2646, 8569, 1105, 4182, 2765, 1473, 117, 1133, 1103, 10311, 6978, 2606, 8468, 17849, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = tokenizer(example)\n",
    "\n",
    "desired_output = []\n",
    "for word_id in encoded.word_ids():\n",
    "    if word_id is not None:\n",
    "        start, end = encoded.word_to_tokens(word_id)\n",
    "        if start == end - 1:\n",
    "            tokens = [start]\n",
    "        else:\n",
    "            tokens = [start, end-1]\n",
    "        if len(desired_output) == 0 or desired_output[-1] != tokens:\n",
    "            desired_output.append(tokens)\n",
    "# desired_output,predict_[1]['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None,\n",
       " {'tokens': ['[CLS]',\n",
       "   'these',\n",
       "   'drugs',\n",
       "   'can',\n",
       "   'also',\n",
       "   'induce',\n",
       "   'l',\n",
       "   '##ys',\n",
       "   '##oso',\n",
       "   '##mal',\n",
       "   'membrane',\n",
       "   'per',\n",
       "   '##me',\n",
       "   '##abi',\n",
       "   '##li',\n",
       "   '##zation',\n",
       "   'and',\n",
       "   'cancer',\n",
       "   'cell',\n",
       "   'death',\n",
       "   ',',\n",
       "   'but',\n",
       "   'the',\n",
       "   'underlying',\n",
       "   'mechanism',\n",
       "   'remains',\n",
       "   'el',\n",
       "   '##usive',\n",
       "   '.',\n",
       "   '[SEP]'],\n",
       "  'labels': ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-disease',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O']})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils_ner'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 14\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      7\u001b[0m     AutoConfig,\n\u001b[1;32m      8\u001b[0m     AutoModelForTokenClassification,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     set_seed,\n\u001b[1;32m     13\u001b[0m )\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils_ner\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NerDataset, Split, get_labels\n\u001b[1;32m     16\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# 固定参数\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils_ner'"
     ]
    }
   ],
   "source": [
    "# coding=utf-8\n",
    "import logging\n",
    "import os\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "from utils_ner import NerDataset, Split, get_labels\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def main():\n",
    "    # 固定参数\n",
    "    model_name_or_path = \"~/bioinfo/P_subject/NLP/biobert/biobertModelWarehouse/model_from_trained/NER/4_combine_lipid_disease_ture_combine_2\"\n",
    "    data_dir = \"~/bioinfo/P_subject/NLP/biobert/datasets/for_train/datasets_from_download/NER/lipid/2_LipidCorpus_Normalized.Name\"\n",
    "    output_dir = \"~/bioinfo/P_subject/NLP/biobert/biobertModelWarehouse/model_from_trained/NER/4_combine_lipid_disease_ture_combine_2_for_test\"\n",
    "    labels_path = None  # 或者 \"path/to/labels.txt\"\n",
    "    max_seq_length = 128\n",
    "    overwrite_cache = False\n",
    "    cache_dir = None\n",
    "    use_fast_tokenizer = False\n",
    "    seed = 42\n",
    "\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    "    )\n",
    "    logger.info(\"Training/evaluation parameters %s\", output_dir)\n",
    "\n",
    "    set_seed(seed)\n",
    "\n",
    "    labels = get_labels(labels_path)\n",
    "    label_map = {i: label for i, label in enumerate(labels)}\n",
    "    num_labels = len(labels)\n",
    "\n",
    "    config = AutoConfig.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        num_labels=num_labels,\n",
    "        id2label=label_map,\n",
    "        label2id={label: i for i, label in enumerate(labels)},\n",
    "        cache_dir=cache_dir,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        cache_dir=cache_dir,\n",
    "        use_fast=use_fast_tokenizer,\n",
    "    )\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        config=config,\n",
    "        cache_dir=cache_dir,\n",
    "    )\n",
    "\n",
    "    def align_predictions(predictions: np.ndarray, label_ids: np.ndarray) -> Tuple[List[int], List[int]]:\n",
    "        preds = np.argmax(predictions, axis=2)\n",
    "        batch_size, seq_len = preds.shape\n",
    "\n",
    "        out_label_list = [[] for _ in range(batch_size)]\n",
    "        preds_list = [[] for _ in range(batch_size)]\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            for j in range(seq_len):\n",
    "                if label_ids[i, j] != nn.CrossEntropyLoss().ignore_index:\n",
    "                    out_label_list[i].append(label_map[label_ids[i][j]])\n",
    "                    preds_list[i].append(label_map[preds[i][j]])\n",
    "\n",
    "        return preds_list, out_label_list\n",
    "\n",
    "    # 固定的训练参数\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        do_predict=True,\n",
    "    )\n",
    "\n",
    "    # Initialize Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "    )\n",
    "\n",
    "    # Predict\n",
    "    test_dataset = NerDataset(\n",
    "        data_dir=data_dir,\n",
    "        tokenizer=tokenizer,\n",
    "        labels=labels,\n",
    "        model_type=config.model_type,\n",
    "        max_seq_length=max_seq_length,\n",
    "        overwrite_cache=overwrite_cache,\n",
    "        mode=Split.test,\n",
    "    )\n",
    "\n",
    "    predictions, label_ids, metrics = trainer.predict(test_dataset)\n",
    "    preds_list, _ = align_predictions(predictions, label_ids)\n",
    "\n",
    "    # Save predictions\n",
    "    output_test_results_file = os.path.join(output_dir, \"test_results.txt\")\n",
    "    with open(output_test_results_file, \"w\") as writer:\n",
    "        logger.info(\"***** Test results *****\")\n",
    "        for key, value in metrics.items():\n",
    "            logger.info(\"  %s = %s\", key, value)\n",
    "            writer.write(\"%s = %s\\n\" % (key, value))\n",
    "\n",
    "    output_test_predictions_file = os.path.join(output_dir, \"test_predictions.txt\")\n",
    "    with open(output_test_predictions_file, \"w\") as writer:\n",
    "        with open(os.path.join(data_dir, \"test.txt\"), \"r\") as f:\n",
    "            example_id = 0\n",
    "            for line in f:\n",
    "                if line.startswith(\"-DOCSTART-\") or line == \"\" or line == \"\\n\":\n",
    "                    writer.write(line)\n",
    "                    if not preds_list[example_id]:\n",
    "                        example_id += 1\n",
    "                elif preds_list[example_id]:\n",
    "                    entity_label = preds_list[example_id].pop(0)\n",
    "                    output_line = line.split()[0] + \" \" + entity_label + \"\\n\"\n",
    "                    writer.write(output_line)\n",
    "                else:\n",
    "                    logger.warning(\n",
    "                        \"Maximum sequence length exceeded: No prediction for '%s'.\", line.split()[0]\n",
    "                    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3_11_gpu_",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
