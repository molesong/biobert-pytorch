{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "dir_name = '/home/data/t200404/bioinfo/P_subject/NLP/biobert/biobert-pytorch/datasets/NER/MetaboliteNER_lipid/raw_data/'\n",
    "# os.chdir(dir_name)\n",
    "filename = 'LipidCorpusBIO.txt'\n",
    "total_filename = dir_name+ filename\n",
    "# print(total_filename)\n",
    "f = open(total_filename, 'r')\n",
    "f = f.read()\n",
    "f_list = f.split('\\n\\n')\n",
    "random.shuffle(f_list)\n",
    "\n",
    "train_index = [0,round(len(f_list)* 0.7)]\n",
    "dev_index = [round(len(f_list)* 0.7)+1, round(len(f_list)* 0.8)]\n",
    "test_index = [round(len(f_list)* 0.8)+1, len(f_list)-1]\n",
    "\n",
    "train_list = f_list[train_index[0]:train_index[1]]\n",
    "dev_list = f_list[dev_index[0]:dev_index[1]]\n",
    "test_list = f_list[test_index[0]:test_index[1]]\n",
    "\n",
    "\n",
    "with open(dir_name+ \"train_dev.txt\", 'w') as ff:\n",
    "    for i in train_list:\n",
    "        ff.write(i+'\\n\\n')\n",
    "\n",
    "with open(dir_name+ \"devel.txt\", 'w') as ff:\n",
    "    for i in dev_list:\n",
    "        ff.write(i+'\\n\\n')\n",
    "\n",
    "with open(dir_name+ \"test.txt\", 'w') as ff:\n",
    "    for i in test_list:\n",
    "        ff.write(i+'\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "dir_name = '../datasets/NER/MetaboliteNER_from_me/'\n",
    "filename = 'self.trainseqs.pydata.csv'\n",
    "total_filename = dir_name+ filename\n",
    "\n",
    "f = open(total_filename, 'r')\n",
    "f = f.read()\n",
    "f_list = f.split('\\n\\n')\n",
    "random.shuffle(f_list)\n",
    "\n",
    "train_index = [0,round(len(f_list)* 0.7)]\n",
    "dev_index = [round(len(f_list)* 0.7)+1, round(len(f_list)* 0.8)]\n",
    "test_index = [round(len(f_list)* 0.8)+1, len(f_list)-1]\n",
    "\n",
    "train_list = f_list[train_index[0]:train_index[1]]\n",
    "dev_list = f_list[dev_index[0]:dev_index[1]]\n",
    "test_list = f_list[test_index[0]:test_index[1]]\n",
    "\n",
    "\n",
    "with open(dir_name+ \"train_dev.txt\", 'w') as ff:\n",
    "    for i in train_list:\n",
    "        ff.write(i+'\\n\\n')\n",
    "\n",
    "with open(dir_name+ \"devel.txt\", 'w') as ff:\n",
    "    for i in dev_list:\n",
    "        ff.write(i+'\\n\\n')\n",
    "\n",
    "with open(dir_name+ \"test.txt\", 'w') as ff:\n",
    "    for i in test_list:\n",
    "        ff.write(i+'\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39481738021171364"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "141133/357464"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def split(all_list, shuffle=False, ratio=0.8):\n",
    "    #按比例随机抽取数据，格式为每行一句话\n",
    "    num = len(all_list)\n",
    "    offset = int(num * ratio)\n",
    "    if num == 0 or offset < 1:\n",
    "        return [], all_list\n",
    "    if shuffle:\n",
    "        random.shuffle(all_list)\n",
    "    train = all_list[:offset]\n",
    "    dev_test = all_list[offset:]\n",
    "\n",
    "    return train, dev_test\n",
    "\n",
    "def write_split(film, train, dev_test):\n",
    "    #将数据分为train、dev_test\n",
    "    infilm = open(film, 'r', encoding='utf-8')\n",
    "    trainfilm = open(train, 'w', encoding='utf-8')\n",
    "    dev_testfilm = open(dev_test, 'w', encoding='utf-8')\n",
    "    list = []\n",
    "    for datas in infilm.readlines():\n",
    "        datas = datas.replace('\\n','')\n",
    "        list.append(datas)\n",
    "    traindatas, dev_testdatas = split(list, shuffle=True, ratio=0.8)\n",
    "    for traindata in traindatas:\n",
    "        trainfilm.write(traindata + '\\n')\n",
    "    for dev_testdata in dev_testdatas:\n",
    "        dev_testfilm.write(dev_testdata + '\\n')\n",
    "\n",
    "    infilm.close()\n",
    "    trainfilm.close()\n",
    "    dev_testfilm.close()\n",
    "\n",
    "def Extract_tag(input_txt, output_txt):\n",
    "    '''\n",
    "        原格式为：vocal--label，如'菜 O'。\n",
    "        只要句子中含有非‘O’标签，就提取该句子。\n",
    "    '''\n",
    "\n",
    "    with open(input_txt,'r' ,encoding='utf-8') as f:\n",
    "        with open(output_txt,'a',encoding='utf-8') as g:\n",
    "            list = []\n",
    "            # list_ =[]\n",
    "            flag = 0\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                if line.isspace() == False:#一句话未结束\n",
    "                    list.append(line.strip())#先将该句子加入list\n",
    "                    # for i,word in enumerate(line):\n",
    "                    #     if word.isspace()==True:#空格\n",
    "                else:#一句话结束，判断list该句子中是否全为‘O’\n",
    "                    # print(list)\n",
    "                    for words in list:\n",
    "                        for i,word in enumerate(words):\n",
    "                            # print(word)\n",
    "                            if word.isspace()==True:\n",
    "                                if words[i+1]!='O':\n",
    "                                    flag=1\n",
    "                                    break#将该list写入新的文件\n",
    "                                else:\n",
    "                                    continue\n",
    "                        if flag == 1:#存在不为‘O’的标签\n",
    "                            break\n",
    "                    if flag == 1:\n",
    "                        for words in list:\n",
    "                            g.write(words+'\\n')\n",
    "                        g.write('\\n')\n",
    "                    flag = 0\n",
    "                    list = []\n",
    "        g.close()\n",
    "    f.close()\n",
    "\n",
    "def Num_tag(input_txt):\n",
    "    '''\n",
    "        统计标签数量，原格式为：vocal--label，如'菜 O'\n",
    "    '''\n",
    "    with open(input_txt, 'r', encoding='utf-8') as f:\n",
    "        # sum = 46364\n",
    "        list = []\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            for i,words in enumerate(line):\n",
    "                if words.isspace()==True or words == '\\t':\n",
    "                    word = line[i+1:].strip()\n",
    "                    if word not in list:\n",
    "                        list.append(word)\n",
    "\n",
    "    # print(list)\n",
    "    # print(len(list))\n",
    "    f.close()\n",
    "    return  list,len(list)\n",
    "\n",
    "def GetWords_Tags(input_txt, output_words_txt, output_tags_txt):\n",
    "    '''\n",
    "        分别获取词和标签，原格式为：vocal--label，如'菜 O'\n",
    "    '''\n",
    "    with open(input_txt, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()#获取所有行\n",
    "        sum = 0\n",
    "        words = []\n",
    "        tags = []\n",
    "        for line in lines:\n",
    "            if line.isspace() == False:#针对一行\n",
    "\n",
    "                for i,word in enumerate(line):\n",
    "                    if word.isspace()==True or word=='  ':\n",
    "                        words.append(line[:i].strip())\n",
    "                        tags.append(line[i:].strip())\n",
    "                        sum += 1\n",
    "                        break\n",
    "            else:#一句话结束\n",
    "                with open(output_words_txt, 'a', encoding='utf-8') as g:\n",
    "                    for word in words:\n",
    "                        g.write(word+' ')\n",
    "                    g.write('\\n')\n",
    "                words = []\n",
    "                with open(output_tags_txt, 'a', encoding='utf-8') as z:\n",
    "                    for tag in tags:\n",
    "                        z.write(tag+' ')\n",
    "                    z.write('\\n')\n",
    "                tags = []\n",
    "\n",
    "        with open(output_words_txt, 'a', encoding='utf-8') as g:\n",
    "            for word in words:\n",
    "                g.write(word + ' ')\n",
    "            g.write('\\n')\n",
    "\n",
    "        with open(output_tags_txt, 'a', encoding='utf-8') as z:\n",
    "            for tag in tags:\n",
    "                z.write(tag + ' ')\n",
    "            z.write('\\n')\n",
    "        # print(words)\n",
    "        # print(tags)\n",
    "        print(sum)\n",
    "\n",
    "    f.close()\n",
    "    g.close()\n",
    "    z.close()\n",
    "\n",
    "def delete_space(input_txt, output_txt):\n",
    "    '''\n",
    "        删除多余空行，每行为一句话\n",
    "    '''\n",
    "\n",
    "    f = open(input_txt, 'r', encoding='utf-8')\n",
    "    g = open(output_txt, 'w', encoding='utf-8')\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        if line.isspace()==False and line!='\\n':\n",
    "            g.write(line)\n",
    "    f.close()\n",
    "    g.close()\n",
    "\n",
    "def NewFileOfWordsAndTags(input_words_txt, input_tags_txt, output_txt):\n",
    "    '''\n",
    "        将词和标签合并到一句话中，输入输出每行都对应一句话。\n",
    "    '''\n",
    "    f = open(input_words_txt, 'r', encoding='utf-8')\n",
    "    g = open(input_tags_txt, 'r', encoding='utf-8')\n",
    "    z = open(output_txt, 'w', encoding='utf-8')\n",
    "    f_lines = f.readlines()\n",
    "    g_lines = g.readlines()\n",
    "    str = []\n",
    "    for i, line in enumerate(f_lines):\n",
    "        if line.isspace() or line == '\\n':\n",
    "            continue\n",
    "        else:\n",
    "            # str.append(line.strip()+g_lines[i].strip())\n",
    "            line = line.strip()\n",
    "            for word in line:\n",
    "                str.append(word)\n",
    "            str.append(' ')\n",
    "            line_ = g_lines[i].strip()\n",
    "            for word_ in line_:\n",
    "                str.append(word_)\n",
    "\n",
    "            for i in str:\n",
    "                z.write(i)\n",
    "            z.write('\\n')\n",
    "            str = []\n",
    "    f.close()\n",
    "    g.close()\n",
    "    z.close()\n",
    "\n",
    "def SplitSentence(input_txt, output_txt):\n",
    "    '''\n",
    "        将一句话中的词和标签分隔开，输入为每行一句话。\n",
    "        输出格式为：vocal--label，如'菜 O'，每行一个词，一句话对应多行。\n",
    "        这种方法有漏洞，需要注意，比如在文章中出现'O'，会把它当作标签\n",
    "    '''\n",
    "    f = open(input_txt, 'r', encoding='utf-8')\n",
    "    g = open(output_txt, 'w', encoding='utf-8')\n",
    "    new_words = []\n",
    "    new_tags = []\n",
    "    new_line = []\n",
    "    for line in f.readlines():  # 针对一句话，包含实体和标签\n",
    "\n",
    "        words = line.strip().split('\\t')  # 针对一句话，这里是空格，也可能是\\t\n",
    "        # print(words)\n",
    "        for i, word in enumerate(words):  # 针对每个词\n",
    "            if word == 'O' or word == 'B':# or word == 'B-PERSON' or word == 'B-TIME' or word == 'B-LOCATION':\n",
    "                new_words = words[:i]\n",
    "                new_tags = words[i:]\n",
    "                \n",
    "                for j, w in enumerate(new_words):\n",
    "                    new_line.append(w + ' ' + new_tags[j])\n",
    "                for label in new_line:\n",
    "                    g.write(label+'\\n')\n",
    "                g.write('\\n')\n",
    "                new_line = []\n",
    "                break\n",
    "\n",
    "    # print(new_line)\n",
    "    f.close()\n",
    "    g.close()\n",
    "\n",
    "def SplitSentence_new(input_txt, output_txt):\n",
    "    f = open(input_txt, 'r', encoding='utf-8')\n",
    "    g = open(output_txt, 'a', encoding='utf-8')\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        new_words,new_tags = line.strip().split('\\t')\n",
    "        new_words = new_words.split()\n",
    "        new_tags = new_tags .split()\n",
    "        # print(new_words )\n",
    "        # print(new_tags )\n",
    "        for i,j in enumerate(new_words ):\n",
    "            g.write(j + ' ' + new_tags [i] + '\\n')\n",
    "        g.write('\\n')\n",
    "\n",
    "    return 0\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # dir_name = '../datasets/NER/MetabolismNER/'\n",
    "    # filename = 'MetabolomicsBIO.txt'\n",
    "    # new_filename = 'new' + filename\n",
    "    # total_filename = dir_name+ filename\n",
    "    input_txt = '../datasets/NER/MetabolismNER/MetabolomicsBIO.txt'\n",
    "    output_txt = '../datasets/NER/MetabolismNER/new_MetabolomicsBIO.txt'\n",
    "    SplitSentence(input_txt, output_txt)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split(all_list, shuffle=False, ratio=0.8):\n",
    "    #按比例随机抽取数据，格式为每行一句话\n",
    "    num = len(all_list)\n",
    "    offset = int(num * ratio)\n",
    "    if num == 0 or offset < 1:\n",
    "        return [], all_list\n",
    "    if shuffle:\n",
    "        random.shuffle(all_list)\n",
    "    train = all_list[:offset]\n",
    "    dev_test = all_list[offset:]\n",
    "\n",
    "    return train, dev_test\n",
    "\n",
    "def write_split(film, train, dev_test):\n",
    "    #将数据分为train、dev_test\n",
    "    infilm = open(film, 'r', encoding='utf-8')\n",
    "    trainfilm = open(train, 'w', encoding='utf-8')\n",
    "    dev_testfilm = open(dev_test, 'w', encoding='utf-8')\n",
    "    list = []\n",
    "    for datas in infilm.readlines():\n",
    "        datas = datas.replace('\\n','')\n",
    "        list.append(datas)\n",
    "    traindatas, dev_testdatas = split(list, shuffle=True, ratio=0.8)\n",
    "    for traindata in traindatas:\n",
    "        trainfilm.write(traindata + '\\n')\n",
    "    for dev_testdata in dev_testdatas:\n",
    "        dev_testfilm.write(dev_testdata + '\\n')\n",
    "\n",
    "    infilm.close()\n",
    "    trainfilm.close()\n",
    "    dev_testfilm.close()\n",
    "\n",
    "def Extract_tag(input_txt, output_txt):\n",
    "    '''\n",
    "        原格式为：vocal--label，如'菜 O'。\n",
    "        只要句子中含有非‘O’标签，就提取该句子。\n",
    "    '''\n",
    "\n",
    "    with open(input_txt,'r' ,encoding='utf-8') as f:\n",
    "        with open(output_txt,'a',encoding='utf-8') as g:\n",
    "            list = []\n",
    "            # list_ =[]\n",
    "            flag = 0\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                if line.isspace() == False:#一句话未结束\n",
    "                    list.append(line.strip())#先将该句子加入list\n",
    "                    # for i,word in enumerate(line):\n",
    "                    #     if word.isspace()==True:#空格\n",
    "                else:#一句话结束，判断list该句子中是否全为‘O’\n",
    "                    # print(list)\n",
    "                    for words in list:\n",
    "                        for i,word in enumerate(words):\n",
    "                            # print(word)\n",
    "                            if word.isspace()==True:\n",
    "                                if words[i+1]!='O':\n",
    "                                    flag=1\n",
    "                                    break#将该list写入新的文件\n",
    "                                else:\n",
    "                                    continue\n",
    "                        if flag == 1:#存在不为‘O’的标签\n",
    "                            break\n",
    "                    if flag == 1:\n",
    "                        for words in list:\n",
    "                            g.write(words+'\\n')\n",
    "                        g.write('\\n')\n",
    "                    flag = 0\n",
    "                    list = []\n",
    "        g.close()\n",
    "    f.close()\n",
    "\n",
    "def Num_tag(input_txt):\n",
    "    '''\n",
    "        统计标签数量，原格式为：vocal--label，如'菜 O'\n",
    "    '''\n",
    "    with open(input_txt, 'r', encoding='utf-8') as f:\n",
    "        # sum = 46364\n",
    "        list = []\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            for i,words in enumerate(line):\n",
    "                if words.isspace()==True or words == '\\t':\n",
    "                    word = line[i+1:].strip()\n",
    "                    if word not in list:\n",
    "                        list.append(word)\n",
    "\n",
    "    # print(list)\n",
    "    # print(len(list))\n",
    "    f.close()\n",
    "    return  list,len(list)\n",
    "\n",
    "def GetWords_Tags(input_txt, output_words_txt, output_tags_txt):\n",
    "    '''\n",
    "        分别获取词和标签，原格式为：vocal--label，如'菜 O'\n",
    "    '''\n",
    "    with open(input_txt, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()#获取所有行\n",
    "        sum = 0\n",
    "        words = []\n",
    "        tags = []\n",
    "        for line in lines:\n",
    "            if line.isspace() == False:#针对一行\n",
    "\n",
    "                for i,word in enumerate(line):\n",
    "                    if word.isspace()==True or word=='  ':\n",
    "                        words.append(line[:i].strip())\n",
    "                        tags.append(line[i:].strip())\n",
    "                        sum += 1\n",
    "                        break\n",
    "            else:#一句话结束\n",
    "                with open(output_words_txt, 'a', encoding='utf-8') as g:\n",
    "                    for word in words:\n",
    "                        g.write(word+' ')\n",
    "                    g.write('\\n')\n",
    "                words = []\n",
    "                with open(output_tags_txt, 'a', encoding='utf-8') as z:\n",
    "                    for tag in tags:\n",
    "                        z.write(tag+' ')\n",
    "                    z.write('\\n')\n",
    "                tags = []\n",
    "\n",
    "        with open(output_words_txt, 'a', encoding='utf-8') as g:\n",
    "            for word in words:\n",
    "                g.write(word + ' ')\n",
    "            g.write('\\n')\n",
    "\n",
    "        with open(output_tags_txt, 'a', encoding='utf-8') as z:\n",
    "            for tag in tags:\n",
    "                z.write(tag + ' ')\n",
    "            z.write('\\n')\n",
    "        # print(words)\n",
    "        # print(tags)\n",
    "        print(sum)\n",
    "\n",
    "    f.close()\n",
    "    g.close()\n",
    "    z.close()\n",
    "\n",
    "def delete_space(input_txt, output_txt):\n",
    "    '''\n",
    "        删除多余空行，每行为一句话\n",
    "    '''\n",
    "\n",
    "    f = open(input_txt, 'r', encoding='utf-8')\n",
    "    g = open(output_txt, 'w', encoding='utf-8')\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        if line.isspace()==False and line!='\\n':\n",
    "            g.write(line)\n",
    "    f.close()\n",
    "    g.close()\n",
    "\n",
    "def NewFileOfWordsAndTags(input_words_txt, input_tags_txt, output_txt):\n",
    "    '''\n",
    "        将词和标签合并到一句话中，输入输出每行都对应一句话。\n",
    "    '''\n",
    "    f = open(input_words_txt, 'r', encoding='utf-8')\n",
    "    g = open(input_tags_txt, 'r', encoding='utf-8')\n",
    "    z = open(output_txt, 'w', encoding='utf-8')\n",
    "    f_lines = f.readlines()\n",
    "    g_lines = g.readlines()\n",
    "    str = []\n",
    "    for i, line in enumerate(f_lines):\n",
    "        if line.isspace() or line == '\\n':\n",
    "            continue\n",
    "        else:\n",
    "            # str.append(line.strip()+g_lines[i].strip())\n",
    "            line = line.strip()\n",
    "            for word in line:\n",
    "                str.append(word)\n",
    "            str.append(' ')\n",
    "            line_ = g_lines[i].strip()\n",
    "            for word_ in line_:\n",
    "                str.append(word_)\n",
    "\n",
    "            for i in str:\n",
    "                z.write(i)\n",
    "            z.write('\\n')\n",
    "            str = []\n",
    "    f.close()\n",
    "    g.close()\n",
    "    z.close()\n",
    "\n",
    "def SplitSentence(input_txt, output_txt):\n",
    "    '''\n",
    "        将一句话中的词和标签分隔开，输入为每行一句话。\n",
    "        输出格式为：vocal--label，如'菜 O'，每行一个词，一句话对应多行。\n",
    "        这种方法有漏洞，需要注意，比如在文章中出现'O'，会把它当作标签\n",
    "    '''\n",
    "    f = open(input_txt, 'r', encoding='utf-8')\n",
    "    g = open(output_txt, 'w', encoding='utf-8')\n",
    "    new_words = []\n",
    "    new_tags = []\n",
    "    new_line = []\n",
    "    for line in f.readlines():  # 针对一句话，包含实体和标签\n",
    "\n",
    "        words = line.strip().split('\\t')  # 针对一句话，这里是空格，也可能是\\t\n",
    "        # print(words)\n",
    "        for i, word in enumerate(words):  # 针对每个词\n",
    "            if word == 'O' or word == 'B':# or word == 'B-PERSON' or word == 'B-TIME' or word == 'B-LOCATION':\n",
    "                new_words = words[:i]\n",
    "                new_tags = words[i:]\n",
    "                \n",
    "                for j, w in enumerate(new_words):\n",
    "                    new_line.append(w + ' ' + new_tags[j])\n",
    "                for label in new_line:\n",
    "                    g.write(label+'\\n')\n",
    "                g.write('\\n')\n",
    "                new_line = []\n",
    "                break\n",
    "\n",
    "    # print(new_line)\n",
    "    f.close()\n",
    "    g.close()\n",
    "\n",
    "def SplitSentence_new(input_txt, output_txt):\n",
    "    f = open(input_txt, 'r', encoding='utf-8')\n",
    "    g = open(output_txt, 'a', encoding='utf-8')\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        new_words,new_tags = line.strip().split('\\t')\n",
    "        new_words = new_words.split()\n",
    "        new_tags = new_tags .split()\n",
    "        # print(new_words )\n",
    "        # print(new_tags )\n",
    "        for i,j in enumerate(new_words ):\n",
    "            g.write(j + ' ' + new_tags [i] + '\\n')\n",
    "        g.write('\\n')\n",
    "\n",
    "    return 0\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # dir_name = '../datasets/NER/MetabolismNER/'\n",
    "    # filename = 'MetabolomicsBIO.txt'\n",
    "    # new_filename = 'new' + filename\n",
    "    # total_filename = dir_name+ filename\n",
    "    input_txt = '../datasets/NER/MetabolismNER/MetabolomicsBIO.txt'\n",
    "    output_txt = '../datasets/NER/MetabolismNER/new_MetabolomicsBIO.txt'\n",
    "    SplitSentence(input_txt, output_txt)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1,2,3,4,5,6]\n",
    "random.shuffle(x)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python37_tsf_3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
