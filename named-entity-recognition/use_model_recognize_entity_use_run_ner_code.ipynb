{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import packages and init classed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import pdb\n",
    "import subprocess\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "from seqeval.metrics import f1_score, precision_score, recall_score\n",
    "from torch import nn\n",
    "\n",
    "# from transformers import AutoTokenizer,AutoModelForCausalLM\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    EvalPrediction,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "from utils_ner__new import NerDataset, Split, get_labels\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "#在下面的代码中，@dataclass 装饰器省略了手动编写的 __init__ 方法，以及通过 self.x 和 self.y 定义的属性。这使得代码更加简洁和易读。\n",
    "#同时，使用 @dataclass 装饰器后，就可以方便地在外部创建数据类的实例并传递参数\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name_or_path: str = field(\n",
    "        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n",
    "    )\n",
    "    config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
    "    )\n",
    "    tokenizer_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
    "    )\n",
    "    use_fast: bool = field(default=False, metadata={\"help\": \"Set this flag to use fast tokenization.\"})\n",
    "    # If you want to tweak more attributes on your tokenizer, you should do it in a distinct script,\n",
    "    # or just modify its tokenizer_config.json.\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n",
    "    )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataTrainingArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "    \"\"\"\n",
    "\n",
    "    data_dir: str = field(\n",
    "        metadata={\"help\": \"The input data dir. Should contain the .txt files for a CoNLL-2003-formatted task.\"}\n",
    "    )\n",
    "    labels: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Path to a file containing all labels. If not specified, CoNLL-2003 labels are used.\"},\n",
    "    )\n",
    "    max_seq_length: int = field(\n",
    "        default=128,\n",
    "        metadata={\n",
    "            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "            \"than this will be truncated, sequences shorter will be padded.\"\n",
    "        },\n",
    "    )\n",
    "    overwrite_cache: bool = field(\n",
    "        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set directory path and arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set root directory\n",
    "root_dir = \"/home/data/t200404/bioinfo/P_subject/NLP/biobert/\"\n",
    "\n",
    "#set other directory\n",
    "# data_path = root_dir + \"datasets/for_train/datasets_from_download/NER/lipid/2_LipidCorpus_Normalized.Name\"\n",
    "# data_path = root_dir + \"datasets/for_train/dir_for_test/only_test\"\n",
    "data_path = root_dir + \"datasets/for_recognize/download_paper_and_use_Auto-CORPus_deal_paper/deal/recognization_test\"\n",
    "labels_path = root_dir + \"datasets/for_train/datasets_from_download/NER/lipid/1_LipidCorpus/labels.txt\"\n",
    "model_path = root_dir + \"biobertModelWarehouse/model_from_trained/NER_add_words_change_split_way/1_LipidCorpus\"\n",
    "# output_path = root_dir + \"biobertModelWarehouse/model_from_trained/NER_add_words_change_split_way/2_LipidCorpus_Normalized.Name\"\n",
    "output_path = root_dir + \"datasets/for_recognize/download_paper_and_use_Auto-CORPus_deal_paper/deal/recognization_result\"\n",
    "\n",
    "model_args = ModelArguments(model_name_or_path=model_path)\n",
    "data_args = DataTrainingArguments(data_dir=data_path, labels=labels_path,max_seq_length = 512)\n",
    "training_args = TrainingArguments(output_dir=output_path, num_train_epochs=5, learning_rate=3e-5,do_predict = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(training_args.seed)\n",
    "\n",
    "# Prepare CONLL-2003 task\n",
    "labels = get_labels(data_args.labels)\n",
    "label_map: Dict[int, str] = {i: label for i, label in enumerate(labels)}\n",
    "num_labels = len(labels)\n",
    "\n",
    "# Load pretrained model and tokenizer\n",
    "#\n",
    "# Distributed training:\n",
    "# The .from_pretrained methods guarantee that only one local process can concurrently\n",
    "# download model & vocab.\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n",
    "    num_labels=num_labels,\n",
    "    id2label=label_map,\n",
    "    label2id={label: i for i, label in enumerate(labels)},\n",
    "    cache_dir=model_args.cache_dir,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n",
    "    cache_dir=model_args.cache_dir,\n",
    "    use_fast=model_args.use_fast,\n",
    ")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
    "    config=config,\n",
    "    cache_dir=model_args.cache_dir,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load train and eval dataset, init trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = (\n",
    "    NerDataset(\n",
    "        data_dir=data_args.data_dir,\n",
    "        data_file_name = 'train.txt',\n",
    "        tokenizer=tokenizer,\n",
    "        labels=labels,\n",
    "        model_type=config.model_type,\n",
    "        max_seq_length=data_args.max_seq_length,\n",
    "        overwrite_cache=data_args.overwrite_cache,\n",
    "        mode=Split.train,\n",
    "    )\n",
    "    if training_args.do_train\n",
    "    else None\n",
    ")\n",
    "eval_dataset = (\n",
    "    NerDataset(\n",
    "        data_dir=data_args.data_dir,\n",
    "        tokenizer=tokenizer,\n",
    "        labels=labels,\n",
    "        model_type=config.model_type,\n",
    "        max_seq_length=data_args.max_seq_length,\n",
    "        overwrite_cache=data_args.overwrite_cache,\n",
    "        mode=Split.dev,\n",
    "    )\n",
    "    if training_args.do_eval\n",
    "    else None\n",
    ")\n",
    "def align_predictions(predictions: np.ndarray, label_ids: np.ndarray) -> Tuple[List[int], List[int]]:\n",
    "    preds = np.argmax(predictions, axis=2)\n",
    "\n",
    "    batch_size, seq_len = preds.shape\n",
    "\n",
    "    out_label_list = [[] for _ in range(batch_size)]\n",
    "    preds_list = [[] for _ in range(batch_size)]\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        for j in range(seq_len):\n",
    "            if label_ids[i, j] != nn.CrossEntropyLoss().ignore_index:\n",
    "                out_label_list[i].append(label_map[label_ids[i][j]])\n",
    "                preds_list[i].append(label_map[preds[i][j]])\n",
    "\n",
    "    return preds_list, out_label_list\n",
    "\n",
    "def compute_metrics(p: EvalPrediction) -> Dict:\n",
    "    preds_list, out_label_list = align_predictions(p.predictions, p.label_ids)\n",
    "    \n",
    "    return {\n",
    "        \"precision\": precision_score(out_label_list, preds_list),\n",
    "        \"recall\": recall_score(out_label_list, preds_list),\n",
    "        \"f1\": f1_score(out_label_list, preds_list),\n",
    "    }\n",
    "\n",
    "# Initialize our Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "dir_ = data_args.data_dir\n",
    "input_file = 'test.txt'\n",
    "test_dataset = NerDataset(\n",
    "        data_dir= dir_,\n",
    "        data_file_name = input_file,\n",
    "        tokenizer=tokenizer,\n",
    "        labels=labels,\n",
    "        model_type=config.model_type,\n",
    "        max_seq_length=data_args.max_seq_length,\n",
    "        overwrite_cache=data_args.overwrite_cache,\n",
    "        mode=Split.test,\n",
    "    )\n",
    "\n",
    "predictions, label_ids, metrics = trainer.predict(test_dataset)\n",
    "preds_list, _ = align_predictions(predictions, label_ids)  #preds_list 是预测结果，对应的是每句话的实体列表，不是tokens\n",
    "\n",
    "output_test_predictions_file = os.path.join(training_args.output_dir, input_file+\"_predictions.txt\")\n",
    "\n",
    "with open(output_test_predictions_file, \"w\") as writer:\n",
    "    with open(os.path.join(dir_, input_file), \"r\") as f:\n",
    "        example_id = 0\n",
    "        for line in f:\n",
    "            if line.startswith(\"-DOCSTART-\") or line == \"\" or line == \"\\n\":\n",
    "                writer.write(line)\n",
    "                if not preds_list[example_id]:\n",
    "                    example_id += 1\n",
    "            elif preds_list[example_id]:\n",
    "                entity_label = preds_list[example_id].pop(0)\n",
    "                output_line = line.split()[0] + \" \" + entity_label + \"\\n\"\n",
    "                writer.write(output_line)\n",
    "            else:\n",
    "                logger.warning(\n",
    "                    \"Maximum sequence length exceeded: No prediction for '%s'.\", line.split()[0]\n",
    "                )\n",
    "files_to_delete = glob.glob(data_args.data_dir + '/*BertTokenizer*')\n",
    "for file in files_to_delete:\n",
    "    os.remove(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipline done. Next is the test code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.convert_ids_to_tokens(test_dataset[0].input_ids)\n",
    "# root_dir + \"datasets/for_train/datasets_from_download/NER/lipid/2_LipidCorpus_Normalized.Name\"\n",
    "training_args.output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if_add_words_in_tokenizer = True\n",
    "# if if_add_words_in_tokenizer:\n",
    "#     added_lipid_list_filename = '/home/data/t200404/bioinfo/P_subject/NLP/biobert/pytorch-biobert/named-entity-recognition/added_lipid_list.txt'\n",
    "#     with open(added_lipid_list_filename, 'r') as f:\n",
    "#         added_lipid_list = f.read().splitlines()\n",
    "#     for lipid in added_lipid_list:\n",
    "#         tokenizer.add_tokens(lipid)\n",
    "#     model.resize_token_embeddings(len(tokenizer))\n",
    "#     model.save_pretrained(model_args.model_name_or_path)\n",
    "#     tokenizer.save_pretrained(model_args.model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"Thus, changes in plasma S1P d16:1 levels, plasma S1P d18:1 levels, plasma MonCer d18:1 levels or plasma LacCer d18:1 levels were inferred to be disease-induced changes in Alzheimer's disease or DLB\"\n",
    "# text_split = text.split()\n",
    "# print(text_split)\n",
    "# with open(data_args.data_dir+ '/' + 'text_.txt', 'w') as f:\n",
    "#     for i in text_split:\n",
    "#         f.write(i+' '+'O'+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "import os\n",
    "import glob\n",
    "\n",
    "dir_ = '/home/data/t200404/bioinfo/P_subject/NLP/biobert/datasets/for_recognize/download_paper_and_use_Auto-CORPus_deal_paper/deal/extract_result_txt'\n",
    "input_file = 'PMC9440283_bioc.json.txt_one_sentence'\n",
    "test_dataset = NerDataset(\n",
    "        data_dir= dir_,\n",
    "        data_file_name = input_file,\n",
    "        tokenizer=tokenizer,\n",
    "        labels=labels,\n",
    "        model_type=config.model_type,\n",
    "        max_seq_length=data_args.max_seq_length,\n",
    "        overwrite_cache=data_args.overwrite_cache,\n",
    "        mode=Split.test,\n",
    "    )\n",
    "\n",
    "predictions, label_ids, metrics = trainer.predict(test_dataset)\n",
    "preds_list, _ = align_predictions(predictions, label_ids)  #preds_list 是预测结果，对应的是每句话的实体列表，不是tokens\n",
    "\n",
    "output_test_predictions_file = os.path.join(training_args.output_dir, input_file+\"_predictions.txt\")\n",
    "\n",
    "with open(output_test_predictions_file, \"w\") as writer:\n",
    "    with open(os.path.join(dir_, input_file), \"r\") as f:\n",
    "        example_id = 0\n",
    "        for line in f:\n",
    "            if line.startswith(\"-DOCSTART-\") or line == \"\" or line == \"\\n\":\n",
    "                writer.write(line)\n",
    "                if not preds_list[example_id]:\n",
    "                    example_id += 1\n",
    "            elif preds_list[example_id]:\n",
    "                entity_label = preds_list[example_id].pop(0)\n",
    "                output_line = line.split()[0] + \" \" + entity_label + \"\\n\"\n",
    "                writer.write(output_line)\n",
    "            else:\n",
    "                logger.warning(\n",
    "                    \"Maximum sequence length exceeded: No prediction for '%s'.\", line.split()[0]\n",
    "                )\n",
    "files_to_delete = glob.glob(dir_ + '/*BertTokenizer*')\n",
    "for file in files_to_delete:\n",
    "    os.remove(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args.output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop prediction\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "dir_ = '/home/data/t200404/bioinfo/P_subject/NLP/biobert/datasets/for_recognize/download_paper_and_use_Auto-CORPus_deal_paper/deal/extract_result_txt'\n",
    "for input_file in os.listdir(dir_):\n",
    "    test_dataset = NerDataset(\n",
    "        data_dir= dir_,\n",
    "        data_file_name = input_file,\n",
    "        tokenizer=tokenizer,\n",
    "        labels=labels,\n",
    "        model_type=config.model_type,\n",
    "        max_seq_length=data_args.max_seq_length,\n",
    "        overwrite_cache=data_args.overwrite_cache,\n",
    "        mode=Split.test,\n",
    "    )\n",
    "\n",
    "    predictions, label_ids, metrics = trainer.predict(test_dataset)\n",
    "    preds_list, _ = align_predictions(predictions, label_ids)  #preds_list 是预测结果，对应的是每句话的实体列表，不是tokens\n",
    "\n",
    "    output_test_predictions_file = os.path.join(training_args.output_dir, input_file+\"_predictions.txt\")\n",
    "\n",
    "    with open(output_test_predictions_file, \"w\") as writer:\n",
    "        with open(os.path.join(dir_, input_file), \"r\") as f:\n",
    "            example_id = 0\n",
    "            for line in f:\n",
    "                if line.startswith(\"-DOCSTART-\") or line == \"\" or line == \"\\n\":\n",
    "                    writer.write(line)\n",
    "                    if not preds_list[example_id]:\n",
    "                        example_id += 1\n",
    "                elif preds_list[example_id]:\n",
    "                    entity_label = preds_list[example_id].pop(0)\n",
    "                    output_line = line.split()[0] + \" \" + entity_label + \"\\n\"\n",
    "                    writer.write(output_line)\n",
    "                else:\n",
    "                    logger.warning(\n",
    "                        \"Maximum sequence length exceeded: No prediction for '%s'.\", line.split()[0]\n",
    "                    )\n",
    "    files_to_delete = glob.glob(dir_ + '/*BertTokenizer*')\n",
    "    for file in files_to_delete:\n",
    "        os.remove(file)\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.join(dir_, input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions, label_ids, metrics = trainer.predict(test_dataset)\n",
    "preds_list, _ = align_predictions(predictions, label_ids)  #preds_list 是预测结果，对应的是每句话的实体列表，不是tokens\n",
    "\n",
    "# output_test_predictions_file = os.path.join(training_args.output_dir, input_file+\"_predictions.txt\")\n",
    "\n",
    "with open(output_test_predictions_file, \"w\") as writer:\n",
    "    with open(os.path.join(dir_, input_file), \"r\") as f:\n",
    "        example_id = 0\n",
    "        for line in f:\n",
    "            if line.startswith(\"-DOCSTART-\") or line == \"\" or line == \"\\n\":\n",
    "                writer.write(line)\n",
    "                if not preds_list[example_id]:\n",
    "                    example_id += 1\n",
    "            elif preds_list[example_id]:\n",
    "                entity_label = preds_list[example_id].pop(0)\n",
    "                output_line = line.split()[0] + \" \" + entity_label + \"\\n\"\n",
    "                writer.write(output_line)\n",
    "            else:\n",
    "                print(len(line.split()))\n",
    "                logger.warning(\n",
    "                    \"Maximum sequence length exceeded: No prediction for '%s'.\", line.split()[0]\n",
    "                )\n",
    "files_to_delete = glob.glob(data_args.data_dir + '/*BertTokenizer*')\n",
    "for file in files_to_delete:\n",
    "    os.remove(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, label_ids, metrics = trainer.predict(test_dataset)\n",
    "preds_list, _ = align_predictions(predictions, label_ids)  #preds_list 是预测结果，对应的是每句话的实体列表，不是tokens\n",
    "\n",
    "output_test_predictions_file = os.path.join(training_args.output_dir, input_file+\"_predictions.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_list, _ = align_predictions(predictions, label_ids)  #preds_list 是预测结果，对应的是每句话的实体列表，不是tokens\n",
    "\n",
    "with open(output_test_predictions_file, \"w\") as writer:\n",
    "    with open(os.path.join(dir_, input_file), \"r\") as f:\n",
    "        example_id = 0\n",
    "        for line in f:\n",
    "            if line.startswith(\"-DOCSTART-\") or line == \"\" or line == \"\\n\":\n",
    "                writer.write(line)\n",
    "                if not preds_list[example_id]:\n",
    "                    example_id += 1\n",
    "            elif preds_list[example_id]:\n",
    "                entity_label = preds_list[example_id].pop(0)\n",
    "                output_line = line.split()[0] + \" \" + entity_label + \"\\n\"\n",
    "                writer.write(output_line)\n",
    "            else:\n",
    "                print(example_id)\n",
    "                # logger.warning(\n",
    "                #     \"Maximum sequence length exceeded: No prediction for '%s'.\", line.split()[0]\n",
    "                # )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_to_delete = glob.glob(data_args.data_dir + '/cached_test_BertTokenizer_max_seq_length_512*')\n",
    "for file in files_to_delete:\n",
    "    os.remove(file)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "open pkl files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "path_ = '/home/data/t200404/bioinfo/P_subject/NLP/biobert/datasets/for_recognize/download_paper_and_use_Auto-CORPus_deal_paper/deal/extract_result/'\n",
    "with open(path_ + 'df_dict.pkl','rb') as f:\n",
    "    df_dict_only_lipid = pickle.load(f)\n",
    "PMC_bioc_name = 'PMC9481132_bioc.json'\n",
    "df_dict_only_lipid[PMC_bioc_name]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "with open('q.txt','w') as f:\n",
    "    for sentence in df_dict_only_lipid[PMC_bioc_name]['split_sentence']:\n",
    "        sentence = sentence.replace('\\n', ' O\\n')\n",
    "        f.write(sentence +'\\n\\n')\n",
    "\n",
    "\n",
    "    # sentence\n",
    "# sentence.replace('\\n', ' O\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3_11_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
