{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import pdb\n",
    "import subprocess\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "from seqeval.metrics import f1_score, precision_score, recall_score\n",
    "from torch import nn\n",
    "\n",
    "# from transformers import AutoTokenizer,AutoModelForCausalLM\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    EvalPrediction,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "from utils_ner import NerDataset, Split, get_labels\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def main():\n",
    "    # 固定参数\n",
    "    model_name_or_path = \"/home/data/t200404/bioinfo/P_subject/NLP/biobert/biobertModelWarehouse/model_from_trained/NER/4_combine_lipid_disease_ture_combine_2/\"\n",
    "    data_dir = \"/home/data/t200404/bioinfo/P_subject/NLP/biobert/biobertModelWarehouse/model_from_trained/NER/4_combine_lipid_disease_ture_combine_2_for_test\"\n",
    "    output_dir = \"/home/data/t200404/bioinfo/P_subject/NLP/biobert/biobertModelWarehouse/model_from_trained/NER/4_combine_lipid_disease_ture_combine_2_for_test\"\n",
    "    labels_path = '/home/data/t200404/bioinfo/P_subject/NLP/biobert/biobertModelWarehouse/model_from_trained/NER/4_combine_lipid_disease_ture_combine_2_for_test/'+ 'labels.txt' # 或者 \"path/to/labels.txt\"\n",
    "    max_seq_length = 384\n",
    "    overwrite_cache = False\n",
    "    cache_dir = None    \n",
    "    use_fast_tokenizer = False\n",
    "    seed = 42\n",
    "\n",
    "\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    "    )\n",
    "    logger.info(\"Training/evaluation parameters %s\", output_dir)\n",
    "\n",
    "    set_seed(seed)\n",
    "\n",
    "    labels = get_labels(labels_path)\n",
    "    label_map = {i: label for i, label in enumerate(labels)}\n",
    "    num_labels = len(labels)\n",
    "\n",
    "\n",
    "    # labels = get_labels(data_args.labels)\n",
    "    label_map: Dict[int, str] = {i: label for i, label in enumerate(labels)}\n",
    "    num_labels = len(labels)\n",
    "\n",
    "    config = AutoConfig.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        num_labels=num_labels,\n",
    "        id2label=label_map,\n",
    "        label2id={label: i for i, label in enumerate(labels)},\n",
    "        cache_dir=cache_dir,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        cache_dir=cache_dir,\n",
    "        use_fast=use_fast_tokenizer,\n",
    "    )\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        config=config,\n",
    "        cache_dir=cache_dir,\n",
    "    )\n",
    "\n",
    "    def align_predictions(predictions: np.ndarray, label_ids: np.ndarray) -> Tuple[List[int], List[int]]:\n",
    "        preds = np.argmax(predictions, axis=2)\n",
    "        batch_size, seq_len = preds.shape\n",
    "\n",
    "        out_label_list = [[] for _ in range(batch_size)]\n",
    "        preds_list = [[] for _ in range(batch_size)]\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            for j in range(seq_len):\n",
    "                if label_ids[i, j] != nn.CrossEntropyLoss().ignore_index:\n",
    "                    out_label_list[i].append(label_map[label_ids[i][j]])\n",
    "                    preds_list[i].append(label_map[preds[i][j]])\n",
    "\n",
    "        return preds_list, out_label_list\n",
    "\n",
    "    # 固定的训练参数\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        do_predict=True,\n",
    "    )\n",
    "\n",
    "    # Initialize Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "    )\n",
    "\n",
    "    # Predict\n",
    "    test_dataset = NerDataset(\n",
    "        data_dir=data_dir,\n",
    "        tokenizer=tokenizer,\n",
    "        labels=labels,\n",
    "        model_type=config.model_type,\n",
    "        max_seq_length=max_seq_length,\n",
    "        overwrite_cache=overwrite_cache,\n",
    "        mode=Split.test,\n",
    "    )\n",
    "\n",
    "    predictions, label_ids, metrics = trainer.predict(test_dataset)\n",
    "    preds_list, _ = align_predictions(predictions, label_ids)\n",
    "\n",
    "    # Save predictions\n",
    "    # output_test_results_file = os.path.join(output_dir, \"test_results.txt\")\n",
    "    # with open(output_test_results_file, \"w\") as writer:\n",
    "    #     logger.info(\"***** Test results *****\")\n",
    "    #     for key, value in metrics.items():\n",
    "    #         logger.info(\"  %s = %s\", key, value)\n",
    "    #         writer.write(\"%s = %s\\n\" % (key, value))\n",
    "\n",
    "    output_test_predictions_file = os.path.join(output_dir, \"test_predictions.txt\")\n",
    "    with open(output_test_predictions_file, \"w\") as writer:\n",
    "        with open(os.path.join(data_dir, \"test.txt\"), \"r\") as f:\n",
    "            example_id = 0\n",
    "            for line in f:\n",
    "                if line.startswith(\"-DOCSTART-\") or line == \"\" or line == \"\\n\":\n",
    "                    writer.write(line)\n",
    "                    if not preds_list[example_id]:\n",
    "                        example_id += 1\n",
    "                elif preds_list[example_id]:\n",
    "                    entity_label = preds_list[example_id].pop(0)\n",
    "                    output_line = line.split()[0] + \" \" + entity_label + \"\\n\"\n",
    "                    writer.write(output_line)\n",
    "                else:\n",
    "                    logger.warning(\n",
    "                        \"Maximum sequence length exceeded: No prediction for '%s'.\", line.split()[0]\n",
    "                    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from seqeval.metrics import f1_score, precision_score, recall_score\n",
    "from torch import nn\n",
    "\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoTokenizer,\n",
    "    EvalPrediction,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "from utils_ner import NerDataset, Split, get_labels\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def main(input_text=None):\n",
    "    # 固定参数\n",
    "    model_name_or_path = \"/home/data/t200404/bioinfo/P_subject/NLP/biobert/biobertModelWarehouse/model_from_trained/NER/4_combine_lipid_disease_ture_combine_2/\"\n",
    "    data_dir = \"/home/data/t200404/bioinfo/P_subject/NLP/biobert/datasets/for_train/datasets_from_download/NER/lipid/2_LipidCorpus_Normalized.Name\"\n",
    "    output_dir = \"/home/data/t200404/bioinfo/P_subject/NLP/biobert/biobertModelWarehouse/model_from_trained/NER/4_combine_lipid_disease_ture_combine_2_for_test\"\n",
    "    labels_path = '/home/data/t200404/bioinfo/P_subject/NLP/biobert/datasets/for_train/datasets_from_download/NER/lipid/2_LipidCorpus_Normalized.Name/'+ 'labels.txt' # 或者 \"path/to/labels.txt\"\n",
    "    max_seq_length = 384\n",
    "    overwrite_cache = False\n",
    "    cache_dir = None\n",
    "    use_fast_tokenizer = False\n",
    "    seed = 42\n",
    "\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    "    )\n",
    "    logger.info(\"Training/evaluation parameters %s\", output_dir)\n",
    "\n",
    "    set_seed(seed)\n",
    "\n",
    "    labels = get_labels(labels_path)\n",
    "    label_map = {i: label for i, label in enumerate(labels)}\n",
    "    num_labels = len(labels)\n",
    "\n",
    "    config = AutoConfig.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        num_labels=num_labels,\n",
    "        id2label=label_map,\n",
    "        label2id={label: i for i, label in enumerate(labels)},\n",
    "        cache_dir=cache_dir,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        cache_dir=cache_dir,\n",
    "        use_fast=use_fast_tokenizer,\n",
    "    )\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        config=config,\n",
    "        cache_dir=cache_dir,\n",
    "    )\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    def align_predictions(predictions: np.ndarray, label_ids: np.ndarray) -> Tuple[List[int], List[int]]:\n",
    "        preds = np.argmax(predictions, axis=2)\n",
    "        batch_size, seq_len = preds.shape\n",
    "\n",
    "        out_label_list = [[] for _ in range(batch_size)]\n",
    "        preds_list = [[] for _ in range(batch_size)]\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            for j in range(seq_len):\n",
    "                if label_ids[i, j] != nn.CrossEntropyLoss().ignore_index:\n",
    "                    out_label_list[i].append(label_map[label_ids[i][j]])\n",
    "                    preds_list[i].append(label_map[preds[i][j]])\n",
    "\n",
    "        return preds_list, out_label_list\n",
    "\n",
    "    def predict_text(input_text):\n",
    "        tokens = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=max_seq_length).to(device)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = model(**tokens)\n",
    "        predictions = output.logits.cpu().numpy()\n",
    "        preds_list, _ = align_predictions(predictions, tokens['input_ids'].cpu().numpy())\n",
    "        return preds_list\n",
    "\n",
    "    # 固定的训练参数\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        do_predict=True,\n",
    "    )\n",
    "\n",
    "    # Initialize Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "    )\n",
    "\n",
    "    # Predict\n",
    "    if input_text:\n",
    "        preds_list = predict_text(input_text)\n",
    "        print(\"Predictions:\", preds_list)\n",
    "    else:\n",
    "        test_dataset = NerDataset(\n",
    "            data_dir=data_dir,\n",
    "            tokenizer=tokenizer,\n",
    "            labels=labels,\n",
    "            model_type=config.model_type,\n",
    "            max_seq_length=max_seq_length,\n",
    "            overwrite_cache=overwrite_cache,\n",
    "            mode=Split.test,\n",
    "        )\n",
    "\n",
    "        predictions, label_ids, metrics = trainer.predict(test_dataset)\n",
    "        preds_list, _ = align_predictions(predictions, label_ids)\n",
    "\n",
    "        # Save predictions\n",
    "        output_test_results_file = os.path.join(output_dir, \"test_results.txt\")\n",
    "        with open(output_test_results_file, \"w\") as writer:\n",
    "            logger.info(\"***** Test results *****\")\n",
    "            for key, value in metrics.items():\n",
    "                logger.info(\"  %s = %s\", key, value)\n",
    "                writer.write(\"%s = %s\\n\" % (key, value))\n",
    "\n",
    "        output_test_predictions_file = os.path.join(output_dir, \"test_predictions.txt\")\n",
    "        with open(output_test_predictions_file, \"w\") as writer:\n",
    "            with open(os.path.join(data_dir, \"test.txt\"), \"r\") as f:\n",
    "                example_id = 0\n",
    "                for line in f:\n",
    "                    if line.startswith(\"-DOCSTART-\") or line == \"\" or line == \"\\n\":\n",
    "                        writer.write(line)\n",
    "                        if not preds_list[example_id]:\n",
    "                            example_id += 1\n",
    "                    elif preds_list[example_id]:\n",
    "                        entity_label = preds_list[example_id].pop(0)\n",
    "                        output_line = line.split()[0] + \" \" + entity_label + \"\\n\"\n",
    "                        writer.write(output_line)\n",
    "                    else:\n",
    "                        logger.warning(\n",
    "                            \"Maximum sequence length exceeded: No prediction for '%s'.\", line.split()[0]\n",
    "                        )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_text = \"Thus, changes in plasma  PC(20:1) levels, plasma S1P d18:1 levels, plasma MonCer d18:1 levels or plasma LacCer d18:1 levels were inferred to be disease-induced changes in Alzheimer's disease or DLB\"  # 在这里输入你的文本\n",
    "    main(input_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/18/2024 16:03:33 - INFO - __main__ -   Training/evaluation parameters /home/data/t200404/bioinfo/P_subject/NLP/biobert/biobertModelWarehouse/model_from_trained/NER/4_combine_lipid_disease_ture_combine_2_for_test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/data/t200404/software/anaconda3/envs/python3_11_gpu/lib/python3.11/site-packages/accelerate/accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [['O', 'O', 'O', 'O', 'O', 'O', 'B-lipid', 'O', 'I-lipid', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-lipid', 'I-lipid', 'I-lipid', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-lipid', 'B-lipid', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-lipid', 'B-lipid', 'O', 'I-disease', 'O', 'I-lipid', 'O', 'O', 'O', 'I-lipid', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-disease', 'I-disease', 'I-disease', 'O', 'I-disease', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-lipid', 'B-lipid', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-lipid', 'B-lipid', 'I-disease', 'B-lipid', 'O', 'I-lipid', 'I-disease', 'I-disease', 'I-disease', 'O', 'I-disease', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-lipid', 'B-lipid', 'B-lipid', 'O', 'O', 'O', 'I-lipid', 'I-lipid', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-disease', 'I-disease', 'I-disease', 'O', 'O', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-lipid', 'B-lipid', 'O', 'I-lipid', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-lipid', 'B-lipid', 'O', 'I-lipid', 'I-lipid', 'I-lipid', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-disease', 'I-disease', 'O', 'O', 'O', 'O', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'O', 'I-disease', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-lipid', 'B-lipid', 'O', 'O', 'I-disease', 'I-disease', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-disease', 'O', 'O', 'O', 'O', 'I-lipid', 'I-lipid', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-lipid', 'I-lipid', 'I-lipid', 'I-lipid', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]\n"
     ]
    }
   ],
   "source": [
    "# coding=utf-8\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from seqeval.metrics import f1_score, precision_score, recall_score\n",
    "from torch import nn\n",
    "\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoTokenizer,\n",
    "    EvalPrediction,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "from utils_ner import NerDataset, Split, get_labels\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class InputExample:\n",
    "    guid: str\n",
    "    words: List[str]\n",
    "    labels: List[str]\n",
    "\n",
    "class CustomNerDataset(NerDataset):\n",
    "    def __init__(self, tokenizer, input_text, labels, max_seq_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_text = input_text\n",
    "        self.labels = labels\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.examples = self._create_examples()\n",
    "\n",
    "    def _create_examples(self):\n",
    "        words = self.input_text.split()\n",
    "        labels = [\"O\"] * len(words)\n",
    "        return [InputExample(guid=\"input_text\", words=words, labels=labels)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        example = self.examples[idx]\n",
    "        tokens = []\n",
    "        label_ids = []\n",
    "        for word, label in zip(example.words, example.labels):\n",
    "            word_tokens = self.tokenizer.tokenize(word)\n",
    "            tokens.extend(word_tokens)\n",
    "            label_ids.extend([self.labels.index(label)] * len(word_tokens))\n",
    "        if len(tokens) > self.max_seq_length - 2:\n",
    "            tokens = tokens[: (self.max_seq_length - 2)]\n",
    "            label_ids = label_ids[: (self.max_seq_length - 2)]\n",
    "        tokens = [self.tokenizer.cls_token] + tokens + [self.tokenizer.sep_token]\n",
    "        label_ids = [self.labels.index(\"O\")] + label_ids + [self.labels.index(\"O\")]\n",
    "        input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "        padding_length = self.max_seq_length - len(input_ids)\n",
    "        input_ids += [self.tokenizer.pad_token_id] * padding_length\n",
    "        attention_mask += [0] * padding_length\n",
    "        label_ids += [self.labels.index(\"O\")] * padding_length\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(label_ids, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "def main(input_text=None):\n",
    "    # 固定参数\n",
    "    model_name_or_path = \"/home/data/t200404/bioinfo/P_subject/NLP/biobert/biobertModelWarehouse/model_from_trained/NER/4_combine_lipid_disease_ture_combine_2/\"\n",
    "    data_dir = \"/home/data/t200404/bioinfo/P_subject/NLP/biobert/datasets/for_train/datasets_from_download/NER/lipid/2_LipidCorpus_Normalized.Name\"\n",
    "    output_dir = \"/home/data/t200404/bioinfo/P_subject/NLP/biobert/biobertModelWarehouse/model_from_trained/NER/4_combine_lipid_disease_ture_combine_2_for_test\"\n",
    "    labels_path = '/home/data/t200404/bioinfo/P_subject/NLP/biobert/datasets/for_train/datasets_from_download/NER/lipid/2_LipidCorpus_Normalized.Name/'+ 'labels.txt' # 或者 \"path/to/labels.txt\"\n",
    "    max_seq_length = 384\n",
    "    overwrite_cache = False\n",
    "    cache_dir = None\n",
    "    use_fast_tokenizer = False\n",
    "    seed = 42\n",
    "\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levellevelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    "    )\n",
    "    logger.info(\"Training/evaluation parameters %s\", output_dir)\n",
    "\n",
    "    set_seed(seed)\n",
    "\n",
    "    labels = get_labels(labels_path)\n",
    "    label_map = {i: label for i, label in enumerate(labels)}\n",
    "    num_labels = len(labels)\n",
    "\n",
    "    config = AutoConfig.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        num_labels=num_labels,\n",
    "        id2label=label_map,\n",
    "        label2id={label: i for i, label in enumerate(labels)},\n",
    "        cache_dir=cache_dir,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        cache_dir=cache_dir,\n",
    "        use_fast=use_fast_tokenizer,\n",
    "    )\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        config=config,\n",
    "        cache_dir=cache_dir,\n",
    "    )\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    def align_predictions(predictions: np.ndarray, label_ids: np.ndarray) -> Tuple[List[int], List[int]]:\n",
    "        preds = np.argmax(predictions, axis=2)\n",
    "        batch_size, seq_len = preds.shape\n",
    "\n",
    "        out_label_list = [[] for _ in range(batch_size)]\n",
    "        preds_list = [[] for _ in range(batch_size)]\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            for j in range(seq_len):\n",
    "                if label_ids[i, j] != nn.CrossEntropyLoss().ignore_index:\n",
    "                    out_label_list[i].append(label_map[label_ids[i][j]])\n",
    "                    preds_list[i].append(label_map[preds[i][j]])\n",
    "\n",
    "        return preds_list, out_label_list\n",
    "\n",
    "    def predict_text(input_text):\n",
    "        # 创建临时数据集\n",
    "        dataset = CustomNerDataset(tokenizer, input_text, labels, max_seq_length)\n",
    "        predictions, label_ids, metrics = trainer.predict(dataset)\n",
    "        preds_list, _ = align_predictions(predictions, label_ids)\n",
    "        return preds_list\n",
    "\n",
    "    # 固定的训练参数\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        do_predict=True,\n",
    "    )\n",
    "\n",
    "    # Initialize Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "    )\n",
    "\n",
    "    # Predict\n",
    "    if input_text:\n",
    "        preds_list = predict_text(input_text)\n",
    "        print(\"Predictions:\", preds_list)\n",
    "    else:\n",
    "        test_dataset = NerDataset(\n",
    "            data_dir=data_dir,\n",
    "            tokenizer=tokenizer,\n",
    "            labels=labels,\n",
    "            model_type=config.model_type,\n",
    "            max_seq_length=max_seq_length,\n",
    "            overwrite_cache=overwrite_cache,\n",
    "            mode=Split.test,\n",
    "        )\n",
    "\n",
    "        predictions, label_ids, metrics = trainer.predict(test_dataset)\n",
    "        preds_list, _ = align_predictions(predictions, label_ids)\n",
    "\n",
    "        # Save predictions\n",
    "        output_test_results_file = os.path.join(output_dir, \"test_results.txt\")\n",
    "        with open(output_test_results_file, \"w\") as writer:\n",
    "            logger.info(\"***** Test results *****\")\n",
    "            for key, value in metrics.items():\n",
    "                logger.info(\"  %s = %s\", key, value)\n",
    "                writer.write(\"%s = %s\\n\" % (key, value))\n",
    "\n",
    "        output_test_predictions_file = os.path.join(output_dir, \"test_predictions.txt\")\n",
    "        with open(output_test_predictions_file, \"w\") as writer:\n",
    "            with open(os.path.join(data_dir, \"test.txt\"), \"r\") as f:\n",
    "                example_id = 0\n",
    "                for line in f:\n",
    "                    if line.startswith(\"-DOCSTART-\") or line == \"\" or line == \"\\n\":\n",
    "                        writer.write(line)\n",
    "                        if not preds_list[example_id]:\n",
    "                            example_id += 1\n",
    "                    elif preds_list[example_id]:\n",
    "                        entity_label = preds_list[example_id].pop(0)\n",
    "                        output_line = line.split()[0] + \" \" + entity_label + \"\\n\"\n",
    "                        writer.write(output_line)\n",
    "                    else:\n",
    "                        logger.warning(\n",
    "                            \"Maximum sequence length exceeded: No prediction for '%s'.\", line.split()[0]\n",
    "                        )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_text = \"Thus, changes in plasma  PC(20:1) levels, plasma S1P d18:1 levels, plasma MonCer d18:1 levels or plasma LacCer d18:1 levels were inferred to be disease-induced changes in Alzheimer's disease or DLB\"  # 在这里输入你的文本\n",
    "    main(input_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/18/2024 16:06:19 - INFO - __main__ -   Training/evaluation parameters /home/data/t200404/bioinfo/P_subject/NLP/biobert/biobertModelWarehouse/model_from_trained/NER/4_combine_lipid_disease_ture_combine_2_for_test\n",
      "/home/data/t200404/software/anaconda3/envs/python3_11_gpu/lib/python3.11/site-packages/accelerate/accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [['O', 'O', 'O', 'O', 'O', 'O', 'B-lipid', 'O', 'I-lipid', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-lipid', 'I-lipid', 'I-lipid', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-lipid', 'B-lipid', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-lipid', 'B-lipid', 'O', 'I-disease', 'O', 'I-lipid', 'O', 'O', 'O', 'I-lipid', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-disease', 'I-disease', 'I-disease', 'O', 'I-disease', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-lipid', 'B-lipid', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-lipid', 'B-lipid', 'I-disease', 'B-lipid', 'O', 'I-lipid', 'I-disease', 'I-disease', 'I-disease', 'O', 'I-disease', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-lipid', 'B-lipid', 'B-lipid', 'O', 'O', 'O', 'I-lipid', 'I-lipid', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-disease', 'I-disease', 'I-disease', 'O', 'O', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-lipid', 'B-lipid', 'O', 'I-lipid', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-lipid', 'B-lipid', 'O', 'I-lipid', 'I-lipid', 'I-lipid', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-disease', 'I-disease', 'O', 'O', 'O', 'O', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'I-disease', 'O', 'I-disease', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-lipid', 'B-lipid', 'O', 'O', 'I-disease', 'I-disease', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-disease', 'O', 'O', 'O', 'O', 'I-lipid', 'I-lipid', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-lipid', 'I-lipid', 'I-lipid', 'I-lipid', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]\n"
     ]
    }
   ],
   "source": [
    "# coding=utf-8\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from seqeval.metrics import f1_score, precision_score, recall_score\n",
    "from torch import nn\n",
    "\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoTokenizer,\n",
    "    EvalPrediction,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "from utils_ner import NerDataset, Split, get_labels\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class InputExample:\n",
    "    guid: str\n",
    "    words: List[str]\n",
    "    labels: List[str]\n",
    "\n",
    "class CustomNerDataset(NerDataset):\n",
    "    def __init__(self, tokenizer, input_text, labels, max_seq_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_text = input_text\n",
    "        self.labels = labels\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.examples = self._create_examples()\n",
    "\n",
    "    def _create_examples(self):\n",
    "        words = self.input_text.split()\n",
    "        labels = [\"O\"] * len(words)\n",
    "        return [InputExample(guid=\"input_text\", words=words, labels=labels)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        example = self.examples[idx]\n",
    "        tokens = []\n",
    "        label_ids = []\n",
    "        for word, label in zip(example.words, example.labels):\n",
    "            word_tokens = self.tokenizer.tokenize(word)\n",
    "            tokens.extend(word_tokens)\n",
    "            label_ids.extend([self.labels.index(label)] * len(word_tokens))\n",
    "        if len(tokens) > self.max_seq_length - 2:\n",
    "            tokens = tokens[: (self.max_seq_length - 2)]\n",
    "            label_ids = label_ids[: (self.max_seq_length - 2)]\n",
    "        tokens = [self.tokenizer.cls_token] + tokens + [self.tokenizer.sep_token]\n",
    "        label_ids = [self.labels.index(\"O\")] + label_ids + [self.labels.index(\"O\")]\n",
    "        input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "        padding_length = self.max_seq_length - len(input_ids)\n",
    "        input_ids += [self.tokenizer.pad_token_id] * padding_length\n",
    "        attention_mask += [0] * padding_length\n",
    "        label_ids += [self.labels.index(\"O\")] * padding_length\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(label_ids, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "def main(input_text=None):\n",
    "    # 固定参数\n",
    "    model_name_or_path = \"/home/data/t200404/bioinfo/P_subject/NLP/biobert/biobertModelWarehouse/model_from_trained/NER/4_combine_lipid_disease_ture_combine_2/\"\n",
    "    data_dir = \"/home/data/t200404/bioinfo/P_subject/NLP/biobert/datasets/for_train/datasets_from_download/NER/lipid/2_LipidCorpus_Normalized.Name\"\n",
    "    output_dir = \"/home/data/t200404/bioinfo/P_subject/NLP/biobert/biobertModelWarehouse/model_from_trained/NER/4_combine_lipid_disease_ture_combine_2_for_test\"\n",
    "    labels_path = '/home/data/t200404/bioinfo/P_subject/NLP/biobert/datasets/for_train/datasets_from_download/NER/lipid/2_LipidCorpus_Normalized.Name/'+ 'labels.txt' # 或者 \"path/to/labels.txt\"\n",
    "    max_seq_length = 384\n",
    "    overwrite_cache = False\n",
    "    cache_dir = None\n",
    "    use_fast_tokenizer = False\n",
    "    seed = 42\n",
    "\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levellevelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    "    )\n",
    "    logger.info(\"Training/evaluation parameters %s\", output_dir)\n",
    "\n",
    "    set_seed(seed)\n",
    "\n",
    "    labels = get_labels(labels_path)\n",
    "    label_map = {i: label for i, label in enumerate(labels)}\n",
    "    num_labels = len(labels)\n",
    "\n",
    "    config = AutoConfig.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        num_labels=num_labels,\n",
    "        id2label=label_map,\n",
    "        label2id={label: i for i, label in enumerate(labels)},\n",
    "        cache_dir=cache_dir,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        cache_dir=cache_dir,\n",
    "        use_fast=use_fast_tokenizer,\n",
    "    )\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        config=config,\n",
    "        cache_dir=cache_dir,\n",
    "    )\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    def align_predictions(predictions: np.ndarray, label_ids: np.ndarray) -> Tuple[List[int], List[int]]:\n",
    "        preds = np.argmax(predictions, axis=2)\n",
    "        batch_size, seq_len = preds.shape\n",
    "\n",
    "        out_label_list = [[] for _ in range(batch_size)]\n",
    "        preds_list = [[] for _ in range(batch_size)]\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            for j in range(seq_len):\n",
    "                if label_ids[i, j] != nn.CrossEntropyLoss().ignore_index:\n",
    "                    out_label_list[i].append(label_map[label_ids[i][j]])\n",
    "                    preds_list[i].append(label_map[preds[i][j]])\n",
    "\n",
    "        return preds_list, out_label_list\n",
    "\n",
    "    def predict_text(input_text):\n",
    "        # 创建临时数据集\n",
    "        dataset = CustomNerDataset(tokenizer, input_text, labels, max_seq_length)\n",
    "        predictions, label_ids, metrics = trainer.predict(dataset)\n",
    "        preds_list, _ = align_predictions(predictions, label_ids)\n",
    "        return preds_list\n",
    "\n",
    "    # 固定的训练参数\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        do_predict=True,\n",
    "    )\n",
    "\n",
    "    # Initialize Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "    )\n",
    "\n",
    "    # Predict\n",
    "    if input_text:\n",
    "        preds_list = predict_text(input_text)\n",
    "        print(\"Predictions:\", preds_list)\n",
    "    else:\n",
    "        test_dataset = NerDataset(\n",
    "            data_dir=data_dir,\n",
    "            tokenizer=tokenizer,\n",
    "            labels=labels,\n",
    "            model_type=config.model_type,\n",
    "            max_seq_length=max_seq_length,\n",
    "            overwrite_cache=overwrite_cache,\n",
    "            mode=Split.test,\n",
    "        )\n",
    "\n",
    "        predictions, label_ids, metrics = trainer.predict(test_dataset)\n",
    "        preds_list, _ = align_predictions(predictions, label_ids)\n",
    "\n",
    "        # Save predictions\n",
    "        output_test_results_file = os.path.join(output_dir, \"test_results.txt\")\n",
    "        with open(output_test_results_file, \"w\") as writer:\n",
    "            logger.info(\"***** Test results *****\")\n",
    "            for key, value in metrics.items():\n",
    "                logger.info(\"  %s = %s\", key, value)\n",
    "                writer.write(\"%s = %s\\n\" % (key, value))\n",
    "\n",
    "        output_test_predictions_file = os.path.join(output_dir, \"test_predictions.txt\")\n",
    "        with open(output_test_predictions_file, \"w\") as writer:\n",
    "            with open(os.path.join(data_dir, \"test.txt\"), \"r\") as f:\n",
    "                example_id = 0\n",
    "                for line in f:\n",
    "                    if line.startswith(\"-DOCSTART-\") or line == \"\" or line == \"\\n\":\n",
    "                        writer.write(line)\n",
    "                        if not preds_list[example_id]:\n",
    "                            example_id += 1\n",
    "                    elif preds_list[example_id]:\n",
    "                        entity_label = preds_list[example_id].pop(0)\n",
    "                        output_line = line.split()[0] + \" \" + entity_label + \"\\n\"\n",
    "                        writer.write(output_line)\n",
    "                    else:\n",
    "                        logger.warning(\n",
    "                            \"Maximum sequence length exceeded: No prediction for '%s'.\", line.split()[0]\n",
    "                        )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_text = \"Thus, changes in plasma  PC(20:1) levels, plasma S1P d18:1 levels, plasma MonCer d18:1 levels or plasma LacCer d18:1 levels were inferred to be disease-induced changes in Alzheimer's disease or DLB\"\n",
    "    main(input_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3_11_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
